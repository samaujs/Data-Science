{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> Elo Merchant Category Recommendation :</h1>\n\nReference : Make Sense Out of Nonsense : ELO EDA","metadata":{"id":"udn5diOPRIHf"}},{"cell_type":"markdown","source":"<h2>Business Problem/Problem Statement :</h2>\n\n> Elo Merchant Category Recommendation is a Kaggle competition which is provided by Elo. As a payment Brand, providing offer promotions and discounts with merchants is a good marketing strategy . Elo needs to keep their customers so loyalty of the customers towards the brand is crucial. For Example, a customer using the Elo card with diverse merchants for a long time, this signifies the user's loyalty is high. To keep the customer as a subscriber, Elo can run different promotional campaign’s targets towards customers with the customer’s favorite or frequently used merchants. These personalized reward programs are planned by the owners of the company to retain existing customers and attract new customers. So, the frequency of using their payment brand should increase. Basically, These programs make the customer’s choice more strongly towards the usage of Elo. The Problem is to find a metric which has to reflect the cardholder’s loyalty with Elo payment brand. Here we have the loyalty score which is a numerical score calculated 2 months after the historical and evaluation period. Elo uses it for their business decision about their promotional campaign.\n\n\n<h2>Dataset Overview :</h2>\n\nThe datasets are largely anonymized, and the meaning of the features are not elaborated. External data is allowed.\n\nThe problem has 5 datasets.\n\n> **train.csv (201917, 6) :** It has 6 features, first_active_month, card-id, feature1, feature2, feature3 and target\n\n> **test.csv (123623, 5) :** The test set has the same features as the train set without targets\n\n> **historical_transactions.csv (29112361, 14) :** Contains up to 3 months worth of historical transactions for each card_id\n\n> **merchants.csv (334696, 22):** Contains the transactions at new merchants(merchant_ids that this particular card_id has not yet visited) over a period of two months.\n\n> **new_merchant_transactions.csv (1963031, 14) :** Two months’ worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data\n\nIn all these datasets, no text data/feature is present. We only have categorical and numerical features. Additionally, by looking at historical_transactions.csv and new_merchant_transactions.csv, we can find that the historical transactions are the transactions occurred before the \"reference date\" and new merchant transactions - the ones that occurred after the reference date (according to the 'month_lag' field, which is generously described as \"month lag to reference date\").\n\n<h2>Mapping the real-world problem to Machine Learning problem :</h2>\n\n> In terms of Machine Learning, we need a metric to measure up the customer's loyalty.A certain loyalty score is assigned for each of the card_id present in train data.\n\n>**Input Features —** Cardholder’s Purchase history, usage time etc.\n\n>**Target Variable —** Loyalty Score\n\n>The Loyalty Score is the target variable for which the Machine Learning Model should be built to predict. **What is loyalty?** According to the Data_Dictionary.xlsx, **loyalty is a numerical score calculated 2 months after historical and evaluation period.** The Loyalty score depends on many aspects of the customers. The purchase history, usage time, merchant’s diversity, etc.  Loyalty scores are real-numbers, It directly gives us the intuition that we have to go for a supervised machine learning regression model to solve this problem where features are as our input in train data and output is real number value which is our predicted loyalty score.\n\n<h2>Real-world constraints :</h2>\n\n> The constraint is that the data which has been provided is not real-customer data. The Provided data is Anonymous and simulated, I think this is due to privacy and legal constraints. Simulated data sometimes has an artificially induced bias which will affect the prediction model performance. We have to deal with this specifically.\n\n<h2>Performance Metric :</h2>\n\n> Root mean square error(RMSE) is used to evaluate our predictions with actual loyalty score. We want our predicted loyalty score close to the actual score. So we need to have a lower RMSE score. This gives us the knowledge that on the basis of input features how close our model makes the predictions as compared to actual predictions.\n","metadata":{"id":"WjHqt3AVRIHh"}},{"cell_type":"markdown","source":"**My understanding of the problem :**\n\n* Based on the data in historical_transactions.csv, Elo picked new mechants to recommend for each card holder.\n* The date when Elo began providing recommentations is called the 'reference date'.\n* The recommended mechant data is not provided (so we don't figure out the recommendation algorithm Elo uses).\n* After the reference date, for each card Elo gathered transaction history for all new merchants that appeared on the card.\n* By comparing each card's new merchant activity and the secret list of the merchants recommended by Elo, the loyalty score was calculated.\n* **The goal is to evaluate Elo's recommendation algorithm by trying to predict in which cases it's going to work well (yielding a high loyalty score) and in which cases - not (yielding a low loyalty score).**","metadata":{"id":"PyCBkKH-aksN"}},{"cell_type":"code","source":"! nvidia-smi -L","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Import all the libraries :**","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras import callbacks\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras import models, layers\n# from keras.optimizers import RMSprop, SGD\nfrom keras.optimizers import Adam, Adadelta\nfrom keras.optimizers.schedules import ExponentialDecay\nfrom keras.metrics import RootMeanSquaredError\n\n#As shown: \n#1) No convolution layer.\n#2) Make the input shape equals to your regression features numbers.\n#3) The network ends with output layer with only 1 output and no activation (it will be a linear layer, applying an activation function would constrain the range the output can take)\n#4) Compile the network with the mse loss function—mean squared error, the square of the difference between the predictions and the targets. This is a widely used loss function for regression problems.\n#5) Make the metric to monitor during training as mean absolute error (MAE). It’s the absolute value of the difference between the predictions and the targets. ","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:02:08.727644Z","iopub.execute_input":"2021-09-29T10:02:08.727978Z","iopub.status.idle":"2021-09-29T10:02:14.409090Z","shell.execute_reply.started":"2021-09-29T10:02:08.727907Z","shell.execute_reply":"2021-09-29T10:02:14.408051Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n# %matplotlib inline\n# %config InlineBackend.figure_format = 'retina'\n# plt.style.use('ggplot')\n\nimport datetime\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport warnings \nwarnings.simplefilter(\"ignore\")\n\n# For computing Variable Inflation Factor (VIF)\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport gc\nimport pickle\n\n# Display columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 150)\npd.set_option('display.max_info_columns', 150)\nprint(pd.get_option(\"display.max_rows\"), pd.get_option(\"display.max_columns\"))\nprint(pd.get_option(\"display.max_info_rows\"), pd.get_option(\"display.max_info_columns\"))","metadata":{"id":"a0aazA45RIHm","execution":{"iopub.status.busy":"2021-09-29T10:02:14.410949Z","iopub.execute_input":"2021-09-29T10:02:14.411252Z","iopub.status.idle":"2021-09-29T10:02:15.038942Z","shell.execute_reply.started":"2021-09-29T10:02:14.411224Z","shell.execute_reply":"2021-09-29T10:02:15.037956Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## **Functions :**","metadata":{}},{"cell_type":"code","source":"## Reference: https://www.kaggle.com/rinnqd/reduce-memory-usage\n\ndef reduce_memory_usage(df, verbose=True):\n  '''\n  This function reduces the memory sizes of dataframe by changing the datatypes of the columns.\n  Parameters\n  df - DataFrame whose size to be reduced\n  verbose - Boolean, to mention the verbose required or not.\n  '''\n  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n  start_mem = df.memory_usage().sum() / 1024**2\n  for col in df.columns:\n      col_type = df[col].dtypes\n      if col_type in numerics:\n          c_min = df[col].min()\n          c_max = df[col].max()\n          if str(col_type)[:3] == 'int':\n              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                  df[col] = df[col].astype(np.int8)\n              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                  df[col] = df[col].astype(np.int16)\n              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                  df[col] = df[col].astype(np.int32)\n              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                  df[col] = df[col].astype(np.int64)\n          else:\n              c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n                  df[col] = df[col].astype(np.float16)\n              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                  df[col] = df[col].astype(np.float32)\n              else:\n                  df[col] = df[col].astype(np.float64)\n  end_mem = df.memory_usage().sum() / 1024**2\n  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n  return df\n\n# Check for missing values : train_data['new_hist_first_buy'].isna().any()\ndef check_missing_values(df):\n    cols_missing_values = []\n    for col in df.columns:\n        if df[col].isna().any():\n            cols_missing_values.append(col)\n            print(col)\n    return cols_missing_values\n            \ndef create_new_columns(name, aggs):\n    # get the individual key from dictionary and the corresponding list of functions for this key\n    # For example : 'purchase_amount' key for functions ['sum','max','min','mean','var']\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n\n# Intersection function\ndef intersection(list1, list2): \n    return list(set(list1) & set(list2))\n\n# Load saved model\ndef load_model_from_picklefile(filename):\n    infile = open(filename,'rb')\n    loaded_model = pickle.load(infile)\n    infile.close()\n    return loaded_model\n\n# Save trained model\ndef save_model_to_picklefile(filename, save_model):\n    model_file = open(filename,'wb')\n    pickle.dump(save_model, model_file)\n    model_file.close()\n\n# Create file for submission to Kaggle\ndef create_file_for_submission(filename, card_ids, final_predictions):    \n    kaggle = pd.DataFrame({'card_id': card_ids, 'target': final_predictions})\n    kaggle.to_csv(filename, index=False)","metadata":{"id":"Tiiadbd6DATL","execution":{"iopub.status.busy":"2021-09-29T10:02:15.040730Z","iopub.execute_input":"2021-09-29T10:02:15.041173Z","iopub.status.idle":"2021-09-29T10:02:15.062296Z","shell.execute_reply.started":"2021-09-29T10:02:15.041128Z","shell.execute_reply":"2021-09-29T10:02:15.061311Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Loading Data :**\n","metadata":{"id":"Lnwl2qLqS-fl"}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/elo-merchant-category-recommendation/train.csv')\ntest_data = pd.read_csv('../input/elo-merchant-category-recommendation/test.csv')\nhistorical_data = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv')\nnewmerchant_data = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv')\nmerchants_data = pd.read_csv('../input/elo-merchant-category-recommendation/merchants.csv')","metadata":{"id":"1DjRBpmES6g3","execution":{"iopub.status.busy":"2021-09-29T10:02:17.665311Z","iopub.execute_input":"2021-09-29T10:02:17.665846Z","iopub.status.idle":"2021-09-29T10:04:01.631283Z","shell.execute_reply.started":"2021-09-29T10:02:17.665804Z","shell.execute_reply":"2021-09-29T10:04:01.630447Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data.shape, test_data.shape, historical_data.shape, newmerchant_data.shape, merchants_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:04:01.632794Z","iopub.execute_input":"2021-09-29T10:04:01.633391Z","iopub.status.idle":"2021-09-29T10:04:01.644751Z","shell.execute_reply.started":"2021-09-29T10:04:01.633334Z","shell.execute_reply":"2021-09-29T10:04:01.643529Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Reduce memory usage of data :**","metadata":{"id":"fdfMGf3KTzXy"}},{"cell_type":"code","source":"train_data = reduce_memory_usage(train_data)\ntest_data = reduce_memory_usage(test_data)\nhistorical_data = reduce_memory_usage(historical_data)\nnewmerchant_data = reduce_memory_usage(newmerchant_data)\nmerchants_data = reduce_memory_usage(merchants_data)","metadata":{"id":"U1wPGpLfTwAa","outputId":"aaeab50b-5338-40cb-85f4-267f9199cbf4","execution":{"iopub.status.busy":"2021-09-29T10:04:01.646867Z","iopub.execute_input":"2021-09-29T10:04:01.647182Z","iopub.status.idle":"2021-09-29T10:08:27.422783Z","shell.execute_reply.started":"2021-09-29T10:04:01.647156Z","shell.execute_reply":"2021-09-29T10:08:27.421458Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## (A) Data Preprocessing and Feature Engineering (Exploratory Data Analysis)","metadata":{}},{"cell_type":"markdown","source":"### Examining the feature statistics :","metadata":{}},{"cell_type":"code","source":"plt.bar([0,1],[train_data.shape[0],test_data.shape[0]])\nplt.xticks([0,1],['train_rows','test_rows'])\n\nprint('The number of rows in train_data is:',train_data.shape[0])\nprint('The number of rows in test_data is:',test_data.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize = (15, 5));\ntrain_data['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='feature_1', rot=0);\ntrain_data['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='feature_2', rot=0);\ntrain_data['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='feature_3', rot=0);\nplt.suptitle('Counts of categories for train features');\n\nfig, ax = plt.subplots(1, 3, figsize = (15, 5));\ntest_data['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='feature_1', rot=0);\ntest_data['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='feature_2', rot=0);\ntest_data['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='feature_3', rot=0);\nplt.suptitle('Counts of categories for test features');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode nominal feature_1, feature_2, feature_3\nohe_train_df_1 = pd.get_dummies(train_data['feature_1'], prefix='f1')\nohe_train_df_2 = pd.get_dummies(train_data['feature_2'], prefix='f2')\nohe_train_df_3 = pd.get_dummies(train_data['feature_3'], prefix='f3')\n\nohe_test_df_1 = pd.get_dummies(test_data['feature_1'], prefix='f1')\nohe_test_df_2 = pd.get_dummies(test_data['feature_2'], prefix='f2')\nohe_test_df_3 = pd.get_dummies(test_data['feature_3'], prefix='f3')\n\nohe_test_df_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for missing values in the loaded data frames","metadata":{}},{"cell_type":"code","source":"check_missing_values(train_data), check_missing_values(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_missing_values(historical_data), check_missing_values(newmerchant_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for card_id[0], 'C_ID_0ab67a22ab' : there are 1304310 - 1304243 = 67 historical transactions\n# for card_id[10], 'C_ID_4859ac9ed5' : there are 23622204 - 23622180 = 24 historical transactions\n# historical_data.loc[23622180:23622204]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_feature_count = 0\nnon_numeric_feature_count = 0\nnumeric_features = []\nnon_numeric_features = []\n\nfor col in historical_data.columns:\n  if (historical_data[col].dtypes == 'object'):\n    non_numeric_feature_count += 1\n    non_numeric_features.append(col)\n    print(\"Non-numeric feature No. {} and name : {}\".format(non_numeric_feature_count, col))\n    print(\"No. of Missing values : {}, Zero values : {} with Mode value : {}\".\\\n          format(historical_data[col].isnull().sum(), (historical_data[col] == 0).sum(),\n                 historical_data[col].mode().values[0]))\n    print(\"*\" * 82)\n    print()\n  else:\n    numeric_feature_count += 1\n    numeric_features.append(col)\n    print(\"Numeric feature No. {} and name : {}\".format(numeric_feature_count, col))\n    # (historical_data[col] == historical_data[col].median()).sum()\n    print(\"No. of Missing values : {}, Zero values : {} with Mode value : {}\".\\\n          format(historical_data[col].isnull().sum(), (historical_data[col] == 0).sum(),\n                 historical_data[col].mode().values[0])) # Get only the value without the index\n    print(\"-\" * 82)\n    print()\n    \n\nprint(\"Total No. of Numeric features {} and Non-numeric features : {}\".format(numeric_feature_count, non_numeric_feature_count))\nprint(\"\\nNumeric features : {} and \\nNon-numeric features : {}\".format(numeric_features, non_numeric_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data['category_2'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Impute missing values (with mean, median, mode) :\n* For historical and new merchant data with mode values","metadata":{}},{"cell_type":"code","source":"# historical_data['category_2'].value_counts()\n# historical_data['category_2'].unique() = [1., nan,  3.,  5.,  2.,  4.]\nhistorical_data_cat2_mode = historical_data['category_2'].mode()\nhistorical_data_merchant_id_mode = historical_data['merchant_id'].mode()\n# historical_data['category_3'].value_counts()\nhistorical_data_cat3_mode = historical_data['category_3'].mode()\n\nprint(\"historical_data mode for category_2 : {}\".format(historical_data_cat2_mode[0]))\nprint(\"historical_data mode for merchant_id : {}\".format(historical_data_merchant_id_mode[0]))\nprint(\"historical_data mode for category_3 : {}\".format(historical_data_cat3_mode[0]))\n\n# Replace missing values with mode values for category_3', 'merchant_id' and 'category_2'\n# When inplace = True, the data is modified in place,\n# which means it will return nothing and the dataframe is now updated.\nfor df in [historical_data, newmerchant_data]:\n    df['category_2'].fillna(historical_data_cat2_mode[0], inplace = True)\n    df['merchant_id'].fillna(historical_data_merchant_id_mode[0], inplace = True)\n    df['category_3'].fillna(historical_data_cat3_mode[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:22:30.745362Z","iopub.execute_input":"2021-09-29T10:22:30.745799Z","iopub.status.idle":"2021-09-29T10:22:47.785151Z","shell.execute_reply.started":"2021-09-29T10:22:30.745758Z","shell.execute_reply":"2021-09-29T10:22:47.784101Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Generalization for 'purchase date' in 'historical data' and 'new merchant data'","metadata":{}},{"cell_type":"code","source":"historical_data['purchase_date'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data['purchase_date'].min(), historical_data['purchase_date'].max(),\\\nhistorical_data['purchase_date'].mode()[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data['purchase_date'].min(), newmerchant_data['purchase_date'].max(),\\\nnewmerchant_data['purchase_date'].mode()[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(historical_data['purchase_date'].min()), type(historical_data['category_2'].mode()[0]) # type(newmerchant_data_purchase_date_min),","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\ncheck_missing_values(historical_data), check_missing_values(newmerchant_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Date operations\n# datetime.datetime.today()\n# (datetime.datetime.today() - newmerchant_data['purchase_date'][0]).days\n# newmerchant_data['purchase_date'][0]\n# type((datetime.datetime.today() - newmerchant_data['purchase_date']).dt.days)\n\n# ((datetime.datetime.today() - df['purchase_date'][0]).dt.days)\n# ((datetime.datetime.today() - newmerchant_data['purchase_date']).dt.days) // 30\n\n# historical_data['purchase_amount'].describe()\n\n# Range of purchase_date : Timestamp('2017-01-01 00:00:08'), Timestamp('2018-02-28 23:59:51')\n# min_date = historical_data['purchase_date'].min() # historical_data['purchase_date'].max()\n# pd.DatetimeIndex(historical_data['purchase_date']).astype(np.int64) * 1e-9\n# historical_data['purchase_date'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observe outliers and replaced with median values :\n* For historical and new merchant data with mode values","metadata":{}},{"cell_type":"code","source":"print('Quantile values for purchase amount in Historical Transaction :')\nprint('25th Percentile :',historical_data['purchase_amount'].quantile(0.25))\nprint('50th Percentile :',historical_data['purchase_amount'].quantile(0.50))\nprint('75th Percentile :',historical_data['purchase_amount'].quantile(0.75))\nprint('100th Percentile :',historical_data['purchase_amount'].quantile(1))\nprint('\\n******************************************************************\\n')\nprint('Quantile values for purchase amount in New Merchant Transaction :')\nprint('25th Percentile :',newmerchant_data['purchase_amount'].quantile(0.25))\nprint('50th Percentile :',newmerchant_data['purchase_amount'].quantile(0.50))\nprint('75th Percentile :',newmerchant_data['purchase_amount'].quantile(0.75))\nprint('100th Percentile :',newmerchant_data['purchase_amount'].quantile(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data[historical_data['purchase_amount']  == 6010603.9717525]","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:06.159333Z","iopub.execute_input":"2021-09-29T10:24:06.159713Z","iopub.status.idle":"2021-09-29T10:24:06.394251Z","shell.execute_reply.started":"2021-09-29T10:24:06.159681Z","shell.execute_reply":"2021-09-29T10:24:06.393156Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"historical_data_outlier_index = historical_data.loc[(historical_data['purchase_amount']  == 6010603.9717525)].index.values\nprint(\"The index of purchase amount in historical data :\", historical_data_outlier_index)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:07.826282Z","iopub.execute_input":"2021-09-29T10:24:07.826683Z","iopub.status.idle":"2021-09-29T10:24:07.866771Z","shell.execute_reply.started":"2021-09-29T10:24:07.826651Z","shell.execute_reply":"2021-09-29T10:24:07.865387Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"newmerchant_data[newmerchant_data['purchase_amount']  == 263.15749789]","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:08.683925Z","iopub.execute_input":"2021-09-29T10:24:08.684326Z","iopub.status.idle":"2021-09-29T10:24:08.716704Z","shell.execute_reply.started":"2021-09-29T10:24:08.684293Z","shell.execute_reply":"2021-09-29T10:24:08.715747Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"newmerchant_data_outlier_index = newmerchant_data.loc[(newmerchant_data['purchase_amount']  == 263.15749789), 'purchase_amount'].index.values\nprint(\"The index of purchase amount in newmerchant data :\", newmerchant_data_outlier_index)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:10.242434Z","iopub.execute_input":"2021-09-29T10:24:10.242861Z","iopub.status.idle":"2021-09-29T10:24:10.254034Z","shell.execute_reply.started":"2021-09-29T10:24:10.242822Z","shell.execute_reply":"2021-09-29T10:24:10.252808Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(\"Original 'Purchase amount' in historical data :\", historical_data.loc[historical_data_outlier_index[0], 'purchase_amount'])\nprint(\"Original 'Purchase amount' in newmerchant data :\", newmerchant_data.loc[newmerchant_data_outlier_index[0], 'purchase_amount'])\n\n# Replace outlier 'purchase amount' with median in historical data\nhistorical_data.loc[historical_data_outlier_index[0], 'purchase_amount'] = historical_data['purchase_amount'].median()\n# Replace outlier 'purchase amount' with median in newmerchant data\nnewmerchant_data.loc[newmerchant_data_outlier_index[0], 'purchase_amount'] = newmerchant_data['purchase_amount'].median()\n\nprint(\"New median 'Purchase amount' in historical data :\", historical_data.loc[historical_data_outlier_index[0], 'purchase_amount'])\nprint(\"New median 'Purchase amount' in newmerchant data :\", newmerchant_data.loc[newmerchant_data_outlier_index[0], 'purchase_amount'])","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:17.232918Z","iopub.execute_input":"2021-09-29T10:24:17.233419Z","iopub.status.idle":"2021-09-29T10:24:18.196858Z","shell.execute_reply.started":"2021-09-29T10:24:17.233378Z","shell.execute_reply":"2021-09-29T10:24:18.195743Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Impute missing value in test data :\n* Replace with the earliest purchase date","metadata":{}},{"cell_type":"code","source":"# df_test['first_active_month'].isna().value_counts()\n# df_test.loc[df_test['first_active_month'].isna()]\n# idx_nan = df_test.loc[df_test['first_active_month'].isna()].index\n# idx_nan.values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_data['first_active_month'].isna().value_counts()\nmissing_card_index = test_data.loc[test_data['first_active_month'].isna(), 'card_id'].index.values\nprint(\"The index of missing card in test data :\", missing_card_index)\n\ntest_data.loc[test_data['first_active_month'].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:28.605493Z","iopub.execute_input":"2021-09-29T10:24:28.605836Z","iopub.status.idle":"2021-09-29T10:24:28.649977Z","shell.execute_reply.started":"2021-09-29T10:24:28.605808Z","shell.execute_reply":"2021-09-29T10:24:28.648866Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"missing_card_id = test_data.loc[test_data['first_active_month'].isna(), 'card_id'].reset_index(drop=True)[0]\n# get the historical data for the missing card_id\ncard_missing_first_active_month = historical_data.loc[historical_data['card_id'] == missing_card_id]\n\nprint(\"Card_id : {} with {} transactions.\".format(missing_card_id,\n                                                  card_missing_first_active_month.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:30.232773Z","iopub.execute_input":"2021-09-29T10:24:30.233206Z","iopub.status.idle":"2021-09-29T10:24:34.833069Z","shell.execute_reply.started":"2021-09-29T10:24:30.233172Z","shell.execute_reply":"2021-09-29T10:24:34.831640Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# card_missing_first_active_month\n# card_missing_first_active_month.columns\n# Earliest 'purchase_date' for card_id 'C_ID_c27b4f80f7' is '2017-03-09'\n# card_missing_first_active_month.sort_values(by=['purchase_date']).iloc[0]\n# card_missing_first_active_month.sort_values(by=['purchase_date']).iloc[0:3, 10] # first three rows in position 10\n# card_missing_first_active_month['purchase_date'].sort_values()\n# card_missing_first_active_month['purchase_date'].value_counts()\n\n# 55 transactions with card_id = 'C_ID_c27b4f80f7' that is without first_active_month\n# historical_data.loc[historical_data['card_id'] == 'C_ID_c27b4f80f7', 'purchase_date']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"card_missing_first_active_month['purchase_date'].min(), card_missing_first_active_month['purchase_date'].max()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:35.575222Z","iopub.execute_input":"2021-09-29T10:24:35.575615Z","iopub.status.idle":"2021-09-29T10:24:35.583464Z","shell.execute_reply.started":"2021-09-29T10:24:35.575582Z","shell.execute_reply":"2021-09-29T10:24:35.582424Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Fill the missing 'first_active_month' in test_data with the earliest value from 'purchase_date' with the card_id\n# Fill the missing value with the mode value '2017-09' in 'first_active_month' of test_data\n# df_test.loc[missing_card_index[0], 'first_active_month'] = '2017-03'\n# df_test.loc[idx_nan.values]['first_active_month'] = '2017-09'\n# df_test.loc[idx_nan.values]['first_active_month']\n\n# format : YYYY-MM\ntest_data['first_active_month'].fillna('2017-03',inplace=True)\ntest_data.loc[missing_card_index[0]]['first_active_month']","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:36.516862Z","iopub.execute_input":"2021-09-29T10:24:36.517312Z","iopub.status.idle":"2021-09-29T10:24:36.540583Z","shell.execute_reply.started":"2021-09-29T10:24:36.517274Z","shell.execute_reply":"2021-09-29T10:24:36.539232Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test_data.loc[test_data['card_id'] == 'C_ID_c27b4f80f7'] # 2017-09","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:41.891025Z","iopub.execute_input":"2021-09-29T10:24:41.891435Z","iopub.status.idle":"2021-09-29T10:24:41.941539Z","shell.execute_reply.started":"2021-09-29T10:24:41.891401Z","shell.execute_reply":"2021-09-29T10:24:41.940267Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Make sure no missing values in df_test\n# df_test['first_active_month'].isna().value_counts()\ncheck_missing_values(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:24:43.587966Z","iopub.execute_input":"2021-09-29T10:24:43.588433Z","iopub.status.idle":"2021-09-29T10:24:43.630072Z","shell.execute_reply.started":"2021-09-29T10:24:43.588388Z","shell.execute_reply":"2021-09-29T10:24:43.628769Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_data['first_active_month'].min(), train_data['first_active_month'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['first_active_month'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['first_active_month'] = pd.to_datetime(train_data['first_active_month'],\n                                                  format='%Y-%m')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (13,5))\n\nsns.lineplot(x = train_data['first_active_month'], y= train_data['target'])\nplt.title(\"Distribution of target over first_active_month\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['first_active_month'].value_counts()[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['first_active_month'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (13,5))\nplt.subplot(121)\nplt.title('Purchase amount (Historical Transaction)');\ntrain_data['first_active_month'].value_counts().plot(kind='hist');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['first_active_month'].min(), test_data['first_active_month'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['first_active_month'] = pd.to_datetime(test_data['first_active_month'],\n                                                  format='%Y-%m')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering\n* Aggregations (Historical, New Merchant and Merchant data)\n* Feature Transformations (with mappings, data type, mean)\n* Feature Engineering with feature creations and subset selections (from 4 to 151)","metadata":{}},{"cell_type":"code","source":"print(\"Earliest purchase date in historical_data :\", historical_data['purchase_date'].min())\nprint(\"Earliest purchase date in newmerchant_data :\", newmerchant_data['purchase_date'].min())\nprint(\"Earliest purchase date in historical_data :\", historical_data['purchase_date'].max())\nprint(\"Earliest purchase date in newmerchant_data :\", newmerchant_data['purchase_date'].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_date_str = newmerchant_data['purchase_date'][0] # '2018-03-11 14:57:36'\npd.to_datetime(sample_date_str) # Timestamp('2018-03-11 14:57:36')\ndatetime.datetime.today() - pd.to_datetime(sample_date_str) # Timedelta('1297 days 12:57:01.779383')\nsample_date_diff_df = (datetime.datetime.today() - pd.to_datetime(newmerchant_data['purchase_date'])).dt.days  # dt for entire dataframe\nsample_date_diff_df[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.to_datetime(newmerchant_data['purchase_date'].max()) - pd.to_datetime(sample_date_str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [historical_data, newmerchant_data]:\n    df_purchase_date_max = pd.to_datetime(df['purchase_date'].max())\n    \n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n    \n    # 7 new columns for historical_data\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    \n    # Replace \"datetime.datetime.today()\" and apply floor division\n    df['month_diff'] = ((df_purchase_date_max - df['purchase_date']).dt.days)//30\n    df['month_diff'] += df['month_lag']\n    \nhistorical_data['purchase_date'].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:28:23.907953Z","iopub.execute_input":"2021-09-29T10:28:23.908412Z","iopub.status.idle":"2021-09-29T10:29:20.955269Z","shell.execute_reply.started":"2021-09-29T10:28:23.908366Z","shell.execute_reply":"2021-09-29T10:29:20.954136Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# 7 new columns for historical_data and newmerchant_data\nhistorical_data.shape, newmerchant_data.shape, train_data.shape, test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggs = {}\n\n# 'dayofweek', 'year'\nfor col in ['month','hour','weekofyear', 'subsector_id','merchant_id','merchant_category_id']:\n            # 'state_id', 'city_id']: # added more features\n    aggs[col] = ['nunique']\n\naggs['purchase_amount'] = ['sum', 'min', 'max', 'mean', 'var', 'median'] # ['sum','max','min','mean','var']\naggs['installments'] = ['sum', 'max', 'mean', 'var'] # 'median', 'min'\naggs['purchase_date'] = ['min', 'max'] # peak-to-peak (maximum - minimum)\naggs['month_lag'] = ['min', 'mean', 'var'] # ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['authorized_flag'] = ['sum', 'mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\n# added more features with median for 'installments' and ptp for 'purchase_date'\n# aggs['month'] = [ 'min', 'max', 'mean', 'var']\n\n# Group based on 'purchase_amount'\nfor col in ['category_2','category_3']:\n    historical_data[col+'_mean'] = historical_data.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']    \n\n# Eg: 'hist_month_nunique', 'hist_hour_nunique'\nnew_columns = create_new_columns('hist',aggs)\nprint(\"New columns :\\n\", new_columns)\ndf_hist_trans_group = historical_data.groupby('card_id').agg(aggs)\n\ndf_hist_trans_group.columns = new_columns\ndf_hist_trans_group.reset_index(drop=False,inplace=True)\ndf_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\ndf_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\ndf_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n\nprint()\nprint(\"Shape of df_hist_trans_group :\\n\", df_hist_trans_group.shape)\nprint(\"df_hist_trans_group columns :\\n\", df_hist_trans_group.columns)\n\ndf_train = train_data.merge(df_hist_trans_group, on='card_id',how='left')\ndf_test = test_data.merge(df_hist_trans_group, on='card_id',how='left')","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:29:20.957192Z","iopub.execute_input":"2021-09-29T10:29:20.957641Z","iopub.status.idle":"2021-09-29T10:30:35.471844Z","shell.execute_reply.started":"2021-09-29T10:29:20.957597Z","shell.execute_reply":"2021-09-29T10:30:35.470623Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# df_train['hist_category_2_mean_mean'].head(5)\n# historical_data['purchase_amount'].head(5)\n# check_missing_values(df_hist_trans_group)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After merging with historical data with 'left' with group by card_id\nprint(df_hist_trans_group.shape, df_train.shape, df_test.shape)\n\n# Release memory\ndel df_hist_trans_group # train_data, test_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:30:35.473865Z","iopub.execute_input":"2021-09-29T10:30:35.474148Z","iopub.status.idle":"2021-09-29T10:30:35.674237Z","shell.execute_reply.started":"2021-09-29T10:30:35.474119Z","shell.execute_reply":"2021-09-29T10:30:35.672955Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\n# train_data['new_hist_first_buy'].isna().any()\ncheck_missing_values(df_train), check_missing_values(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:30:35.675878Z","iopub.execute_input":"2021-09-29T10:30:35.676162Z","iopub.status.idle":"2021-09-29T10:30:35.779972Z","shell.execute_reply.started":"2021-09-29T10:30:35.676136Z","shell.execute_reply":"2021-09-29T10:30:35.778937Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# ((201917, 50), (123623, 49))\ntrain_data.shape, test_data.shape, df_train.shape, df_test.shape, historical_data.shape, newmerchant_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:30:35.781251Z","iopub.execute_input":"2021-09-29T10:30:35.781558Z","iopub.status.idle":"2021-09-29T10:30:35.788547Z","shell.execute_reply.started":"2021-09-29T10:30:35.781530Z","shell.execute_reply":"2021-09-29T10:30:35.787287Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#### Exploring New Merchant data","metadata":{}},{"cell_type":"code","source":"# There are no duplicate card_ids in train_data with 201,917 transactions\ntrain_data_cards = train_data['card_id']\ntrain_data_cards.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_cards = test_data['card_id']\n# There are no duplicate card_ids in train_data with 123,623 transactions\ntest_data_cards.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of unique cards in train_data and test_data :\", (train_data_cards.nunique() + test_data_cards.nunique()))\n\nhistorical_data_cards = historical_data['card_id']\n# There are no duplicate card_ids in train_data with 123,623 transactions\nhistorical_data_cards.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Note :\n> There are NO intersected values between train and test sets\n\n> All unique card_ids in train_data and test_data can be found in historical_data\n\n> All unique card_ids in train_data and test_data can be found in newmerchant_data","metadata":{}},{"cell_type":"code","source":"# Total number of unique cards in train_data and test_data = Total of unique cards in historical_data\nintersect_list = intersection(train_data_cards, test_data_cards)\nprint(\"There are NO intersected card_id :\", intersect_list)\n\n# np.intersect1d(train_data_cards, test_data_cards)\nintersect_list = list(set(train_data_cards) & set(test_data_cards))\nprint(\"No. of intersected card_ids :\", len(intersect_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  pandas.core.series.Series\n# s1 = pd.Series([4, 5, 20, 42, 42, 43])\n# s2 = pd.Series([1, 2, 3, 5, 42])\n# set(s1)&set(s2) same results\n# np.intersect1d(s1, s2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Difference in card_ids\ncard_difference = set(train_data_cards).symmetric_difference(test_data_cards)\nprint(\"Total no. of card_ids in train_data and test_data which are unique : {}\".format(len(card_difference)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"union_cards_df = pd.concat([train_data_cards, test_data_cards])\n\n# type(union_cards_df) is pandas.core.series.Series\n# union_data_size = train_data_cards.nunique() + test_data_cards.nunique() = 325,540\n# The card_ids in historical_data intersects with all the card_ids found in train and test sets\n# intersect_list = intersection(historical_data_cards, union_cards_df)\n# len(intersect_list)\n\ncard_difference = set(historical_data_cards).symmetric_difference(union_cards_df)\nprint(\"The card_ids in train and test but NOT found in historical_data : {}\".format(len(card_difference)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# union_data_size = train_data_cards.nunique() + test_data_cards.nunique() = 325,540\n# Union returns a new set with elements from the set and the specified iterables\ncard_union = set(train_data_cards).union(test_data_cards)\nprint(\"Total no. of unique cards : \", len(card_union))\n\n# symmetric_difference returns a new set with elements in either the set or the specified iterable but not both.\n# The card_ids in historical_data can be found in train and test sets; there are no difference in card_ids\n# card_difference = set(historical_data_cards).symmetric_difference(union_cards_df)\ncard_difference = card_union.symmetric_difference(historical_data_cards)\nprint(\"Unique cards of train and test data but not in historical data : \", len(card_difference))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data_cards = newmerchant_data['card_id']\n# There are 290,001 unique cards in newmerchant_data but train_data has 201,917 cards\nprint(\"No. of unique cards in train_data :\", train_data_cards.nunique())\nprint(\"No. of unique cards in newmerchant_data :\", newmerchant_data_cards.nunique())\n\nintersect_list_train_newmerchant = intersection(train_data_cards, newmerchant_data_cards)\nprint(\"No. of intersected card_ids between train_data_cards and newmerchant_data_cards :\", len(intersect_list_train_newmerchant))\n\n# These cards create NaNs during merging\nprint(\"Unique card_ids in train_data but not in the newmerchant_data : \",\n      train_data_cards.nunique() - len(intersect_list_train_newmerchant))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"card_difference = set(train_data_cards).symmetric_difference(newmerchant_data_cards)\nprint(\"Unique card_ids not in the intersection of train_data and the newmerchant_data : \", len(card_difference))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are 290,001 unique cards in newmerchant_data but train_data has 201,917 cards\nprint(\"No. of unique cards in test_data :\", test_data_cards.nunique())\nprint(\"No. of unique cards in newmerchant_data :\", newmerchant_data_cards.nunique())\n\nintersect_list_test_newmerchant = intersection(test_data_cards, newmerchant_data_cards)\nprint(\"No. of intersected card_ids between test_data_cards and newmerchant_data_cards :\", len(intersect_list_test_newmerchant))\n\n# These cards create NaNs during merging\nprint(\"Unique card_ids in test_data but not in the newmerchant_data : \",\n      test_data_cards.nunique() - len(intersect_list_test_newmerchant))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_newmerchant_card_union = set(intersect_list_train_newmerchant).union(intersect_list_test_newmerchant)\n# len(intersect_list_train_newmerchant) + len(intersect_list_test_newmerchant) gives same result of 290,001\nlen(train_test_newmerchant_card_union)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"card_difference = train_test_newmerchant_card_union.symmetric_difference(newmerchant_data_cards)\nprint(\"Verify that intersected unique cards of train and test data with newmerchant data but not in newmerchant data : \", len(card_difference))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Exploring Merchant data","metadata":{}},{"cell_type":"code","source":"merchants_data_ids = merchants_data['merchant_id']\nnewmerchants_data_ids = newmerchant_data['merchant_id']\n\n# There are 334,633 merchant_id in merchants_data but newmerchant_data has only 226,129 merchant_id\nprint(\"No. of unique merchant_id in merchants_data :\", merchants_data_ids.nunique())\nprint(\"No. of unique merchant_id in newmerchant_data :\", newmerchants_data_ids.nunique())\n\nintersect_list_newmerchants_merchants = intersection(newmerchants_data_ids, merchants_data_ids)\nprint(\"No. of intersected merchant_ids between newmerchants_data_ids and merchants_data_ids :\", len(intersect_list_newmerchants_merchants))\n\n# All merchant_id in newmerchant_data can be found in merchants_data\nprint(\"Unique merchant_ids in newmerchants_data but not in the merchants_data :\",\n      newmerchants_data_ids.nunique() - len(intersect_list_newmerchants_merchants))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data_cat_ids = merchants_data['merchant_category_id']\nnewmerchants_data_cat_ids = newmerchant_data['merchant_category_id']\n\n# There are 334,633 merchant_id in merchants_data but newmerchant_data has only 226,129 merchant_id\nprint(\"No. of unique merchant_category_id in merchants_data :\", merchants_data_cat_ids.nunique())\nprint(\"No. of unique merchant_category_id in newmerchant_data :\", newmerchants_data_cat_ids.nunique())\n\nintersect_list_newmerchants_merchants_cat_id = intersection(newmerchants_data_cat_ids, merchants_data_cat_ids)\nprint(\"No. of intersected merchant_category_ids between newmerchants_data_cat_ids and merchants_data_cat_ids :\",\n      len(intersect_list_newmerchants_merchants_cat_id))\n\n# These merchant_category_id create NaNs during merging\nprint(\"Unique merchant_category_ids in newmerchants_data but not in the merchants_data :\",\n      newmerchants_data_cat_ids.nunique() - len(intersect_list_newmerchants_merchants_cat_id))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data_ids = merchants_data['merchant_id']\nhistorical_data_merchant_ids = historical_data['merchant_id']\n\n# There are 334,633 merchant_id in merchants_data but newmerchant_data has only 226,129 merchant_id\nprint(\"No. of unique merchant_id in merchants_data :\", merchants_data_ids.nunique())\nprint(\"No. of unique merchant_id in historical_data :\", historical_data_merchant_ids.nunique())\n\nintersect_list_historical_merchants = intersection(historical_data_merchant_ids, merchants_data_ids)\nprint(\"No. of intersected merchant_ids between historical_data_merchant_ids and merchants_data_ids :\",\n      len(intersect_list_historical_merchants))\n\n# All merchant_id in historical_data can be found in merchants_data\nprint(\"Unique merchant_ids in historical_data but not in the merchants_data :\",\n      historical_data_merchant_ids.nunique() - len(intersect_list_historical_merchants))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data_cat_ids = merchants_data['merchant_category_id']\nhistorical_data_cat_ids = historical_data['merchant_category_id']\n\n# There are 334,633 merchant_id in merchants_data but newmerchant_data has only 226,129 merchant_id\nprint(\"No. of unique merchant_category_id in merchants_data :\", merchants_data_cat_ids.nunique())\nprint(\"No. of unique merchant_category_id in historical_data :\", historical_data_cat_ids.nunique())\n\nintersect_list_historical_merchants_cat_id = intersection(historical_data_cat_ids, merchants_data_cat_ids)\nprint(\"No. of intersected merchant_category_ids between historical_data_cat_ids and merchants_data_cat_ids :\",\n      len(intersect_list_historical_merchants_cat_id))\n\n# These merchant_category_id create NaNs during merging\nprint(\"Unique merchant_category_ids in historical_data but not in the merchants_data :\",\n      historical_data_cat_ids.nunique() - len(intersect_list_historical_merchants_cat_id))\n\nprint()\nprint(\"There are MORE merchant_category_ids in historical_data than in the merchants_data :\",\n      (historical_data_cat_ids.nunique() - merchants_data_cat_ids.nunique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data['merchant_id'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_test.shape, newmerchant_data.shape, merchants_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:31:38.517793Z","iopub.execute_input":"2021-09-29T10:31:38.518190Z","iopub.status.idle":"2021-09-29T10:31:38.528219Z","shell.execute_reply.started":"2021-09-29T10:31:38.518157Z","shell.execute_reply":"2021-09-29T10:31:38.527076Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"check_missing_values(newmerchant_data), check_missing_values(merchants_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are duplicate values of 'merchant_id'\nmerchants_data['merchant_id'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Impute missing values in merchants data with respective mode values","metadata":{}},{"cell_type":"code","source":"print(merchants_data['avg_sales_lag3'].isna().value_counts())\nprint()\nprint(\"Mode value of avg_sales_lag3 :\", merchants_data['avg_sales_lag3'].mode()[0])\n\nmissing_avg_sales_l3_index = merchants_data.loc[merchants_data['avg_sales_lag3'].isna(), 'merchant_id'].index.values\nprint(\"The index of missing avg_sales_lag3 in merchants_data :\", missing_avg_sales_l3_index)\n\nmerchants_data.loc[merchants_data['avg_sales_lag3'].isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill missing values in merchants_data with respective mode values\nmerchants_data['avg_sales_lag3'].fillna(merchants_data['avg_sales_lag3'].mode()[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:32:03.892175Z","iopub.execute_input":"2021-09-29T10:32:03.892665Z","iopub.status.idle":"2021-09-29T10:32:03.905115Z","shell.execute_reply.started":"2021-09-29T10:32:03.892622Z","shell.execute_reply":"2021-09-29T10:32:03.904066Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(merchants_data['avg_sales_lag6'].isna().value_counts())\nprint()\nprint(\"Mode value of avg_sales_lag6 :\", merchants_data['avg_sales_lag6'].mode()[0])\n\nmissing_avg_sales_l6_index = merchants_data.loc[merchants_data['avg_sales_lag6'].isna(), 'merchant_id'].index.values\nprint(\"The index of missing avg_sales_lag6 in merchants_data :\", missing_avg_sales_l6_index)\n\nmerchants_data.loc[merchants_data['avg_sales_lag6'].isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data['avg_sales_lag6'].fillna(merchants_data['avg_sales_lag6'].mode()[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:32:06.531605Z","iopub.execute_input":"2021-09-29T10:32:06.532092Z","iopub.status.idle":"2021-09-29T10:32:06.544275Z","shell.execute_reply.started":"2021-09-29T10:32:06.532019Z","shell.execute_reply":"2021-09-29T10:32:06.542945Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print(merchants_data['avg_sales_lag12'].isna().value_counts())\nprint()\nprint(\"Mode value of avg_sales_lag12 :\", merchants_data['avg_sales_lag12'].mode()[0])\n\nmissing_avg_sales_l12_index = merchants_data.loc[merchants_data['avg_sales_lag12'].isna(), 'merchant_id'].index.values\nprint(\"The index of missing avg_sales_lag12 in merchants_data :\", missing_avg_sales_l12_index)\n\nmerchants_data.loc[merchants_data['avg_sales_lag12'].isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data['avg_sales_lag12'].fillna(merchants_data['avg_sales_lag12'].mode()[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:32:12.361235Z","iopub.execute_input":"2021-09-29T10:32:12.361937Z","iopub.status.idle":"2021-09-29T10:32:12.373936Z","shell.execute_reply.started":"2021-09-29T10:32:12.361892Z","shell.execute_reply":"2021-09-29T10:32:12.372548Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(merchants_data['category_2'].isna().value_counts())\nprint()\nprint(\"Mode value of category_2 :\", merchants_data['category_2'].mode()[0])\n\nmissing_category_2_index = merchants_data.loc[merchants_data['category_2'].isna(), 'merchant_id'].index.values\nprint(\"The index of missing category_2 in merchants_data :\", missing_category_2_index)\n\nmerchants_data.loc[missing_category_2_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data['category_2'].fillna(merchants_data['category_2'].mode()[0], inplace = True)\nmerchants_data.loc[3:15]","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:32:17.959004Z","iopub.execute_input":"2021-09-29T10:32:17.959538Z","iopub.status.idle":"2021-09-29T10:32:18.002642Z","shell.execute_reply.started":"2021-09-29T10:32:17.959505Z","shell.execute_reply":"2021-09-29T10:32:18.001910Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"check_missing_values(merchants_data), merchants_data.shape, newmerchant_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:32:29.236601Z","iopub.execute_input":"2021-09-29T10:32:29.237230Z","iopub.status.idle":"2021-09-29T10:32:29.416659Z","shell.execute_reply.started":"2021-09-29T10:32:29.237199Z","shell.execute_reply":"2021-09-29T10:32:29.415932Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#### Merging strategy :\n> 1. Aggregate historical data with ***'card_id'*** and merge to train and test with groupby ***'card_id'***\n> 2. Aggregate merchant data with ***'merchant_id'*** and merge to new merchant data with groupby ***'merchant_id'***\n> 3. Merge new merchant data with merchant data to new train and test with groupby ***'card_id'***","metadata":{}},{"cell_type":"code","source":"# merchants_data['most_recent_sales_range'].value_counts()\n\n# merchants_data['most_recent_purchases_range'].value_counts()\n# merchants_data['most_recent_purchases_range'].min(), merchants_data['most_recent_purchases_range'].max()\n\n# merchants_data['most_recent_sales_range'].mode()\n\n# aggs = {}\n# aggs['most_recent_sales_range'] = ['min']\n# md_tmp = merchants_data.groupby('merchant_id').agg(aggs)\n# md_tmp.shape\n\n# md_tmp = pd.DataFrame()\n# Group based on 'purchase_amount'\n# for col in ['most_recent_sales_range','most_recent_purchases_range']:\n#     md_tmp[col+'_avg_sales_lag3_mean'] = merchants_data.groupby([col])['avg_sales_lag3'].transform('min')\n#     md_tmp[col+'_avg_sales_lag6_mean'] = merchants_data.groupby([col])['avg_sales_lag6'].transform('min')\n#     md_tmp[col+'_avg_sales_lag12_mean'] = merchants_data.groupby([col])['avg_sales_lag12'].transform('min')\n\n# md_tmp.reset_index(drop=False,inplace=True)\n# md_tmp.head(2)\n# check_missing_values(md_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature mappings","metadata":{}},{"cell_type":"code","source":"for df in [merchants_data]:\n    df['most_recent_sales_range'] = df['most_recent_sales_range'].map({'A':5, 'B':4, 'C':3, 'D':2, 'E':1})\n    df['most_recent_purchases_range'] = df['most_recent_purchases_range'].map({'A':5, 'B':4, 'C':3, 'D':2, 'E':1})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0})","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:35:53.000951Z","iopub.execute_input":"2021-09-29T10:35:53.001752Z","iopub.status.idle":"2021-09-29T10:35:53.157318Z","shell.execute_reply.started":"2021-09-29T10:35:53.001706Z","shell.execute_reply":"2021-09-29T10:35:53.156183Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Majority revenue and quantity of transactions in last active month are high in merchants data\ndf['most_recent_sales_range'].value_counts(), df['most_recent_purchases_range'].value_counts(), df['category_1'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:35:56.537052Z","iopub.execute_input":"2021-09-29T10:35:56.537443Z","iopub.status.idle":"2021-09-29T10:35:56.559908Z","shell.execute_reply.started":"2021-09-29T10:35:56.537409Z","shell.execute_reply":"2021-09-29T10:35:56.558862Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"aggs = {}\n\nfor col in ['merchant_group_id','subsector_id','category_1','category_4','city_id', 'state_id']: # 'merchant_id'\n    aggs[col] = ['nunique']\n\naggs['numerical_1'] = ['sum', 'mean']\naggs['numerical_2'] = ['sum', 'mean']\naggs['avg_sales_lag3'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['avg_purchases_lag3'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['active_months_lag3'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['avg_sales_lag6'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['avg_purchases_lag6'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['active_months_lag6'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['avg_sales_lag12'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['avg_purchases_lag12'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['active_months_lag12'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['category_2'] = ['mean']\n# Explore more features\naggs['most_recent_sales_range'] = ['min', 'max'] \naggs['most_recent_purchases_range'] = ['min', 'max']\n\naggs['category_1'] = ['min', 'max']\n\nnew_columns = create_new_columns('merchants',aggs)\nprint(\"New columns :\\n\", new_columns)\ndf_merchant_id_group = merchants_data.groupby('merchant_id').agg(aggs) # merchant_category_id\n\ndf_merchant_id_group.columns = new_columns\ndf_merchant_id_group.reset_index(drop=False,inplace=True)\n\nprint(\"Shape of df_merchant_category :\\n\", df_merchant_id_group.shape)\nprint(\"df_merchant_id_group columns :\\n\", df_merchant_id_group.columns)\ndf_merchant_id_group","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:37:33.659537Z","iopub.execute_input":"2021-09-29T10:37:33.659897Z","iopub.status.idle":"2021-09-29T10:37:37.218467Z","shell.execute_reply.started":"2021-09-29T10:37:33.659867Z","shell.execute_reply":"2021-09-29T10:37:37.217473Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Group newmerchant_data and merchants_data\nnewmerchant_with_merchants_data = newmerchant_data.merge(df_merchant_id_group, on='merchant_id',how='left') # merchant_category_id\nnewmerchant_with_merchants_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:37:43.614601Z","iopub.execute_input":"2021-09-29T10:37:43.614985Z","iopub.status.idle":"2021-09-29T10:37:54.829772Z","shell.execute_reply.started":"2021-09-29T10:37:43.614954Z","shell.execute_reply":"2021-09-29T10:37:54.828464Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_test.shape, newmerchant_data.shape, merchants_data.shape, df_merchant_id_group.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:37:54.831763Z","iopub.execute_input":"2021-09-29T10:37:54.832070Z","iopub.status.idle":"2021-09-29T10:37:54.839622Z","shell.execute_reply.started":"2021-09-29T10:37:54.832037Z","shell.execute_reply":"2021-09-29T10:37:54.838327Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"#### When merging with 'merchant_category_id', there are infinite values that should be replaced","metadata":{}},{"cell_type":"code","source":"# np.isinf(df_merchant_id_group).any()\n# np.isinf(df_merchant_id_group).any().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"printing column name where infinity is present\")\n# col_name = df_merchant_id_group.columns.to_series()[np.isinf(df_merchant_id_group).any()]\n\n# print(col_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(df_merchant_id_group['merchants_avg_purchases_lag3_sum'].median())\n# print(df_merchant_id_group['merchants_avg_purchases_lag3_mean'].median())\n# print(df_merchant_id_group['merchants_avg_purchases_lag6_sum'].median())\n# print(df_merchant_id_group['merchants_avg_purchases_lag6_mean'].median())\n# print(df_merchant_id_group['merchants_avg_purchases_lag12_sum'].median())\n# print(df_merchant_id_group['merchants_avg_purchases_lag12_mean'].median())\n\n# missing_value_index = df_merchant_id_group.loc[np.isinf(df_merchant_id_group['merchants_avg_purchases_lag3_sum']),\n#                                                'merchants_avg_purchases_lag3_sum'].index.values\n\n# missing_value_index\n\n# df_merchant_id_group.loc[missing_value_index, 'merchants_avg_purchases_lag3_sum']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace all infinite values to NaN which will replaced with median values collectively\n# df_merchant_id_group.replace([np.inf, -np.inf], np.nan, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merchants_data['merchant_id'].describe()\n\n# newmerchant_with_merchants_data.columns\n\n# merchants_data['merchant_category_id'].nunique(), newmerchant_data['merchant_category_id'].nunique()\n\n# newmerchant_with_merchants_data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_missing_values(newmerchant_with_merchants_data), newmerchant_with_merchants_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:38:16.819765Z","iopub.execute_input":"2021-09-29T10:38:16.820140Z","iopub.status.idle":"2021-09-29T10:38:17.487958Z","shell.execute_reply.started":"2021-09-29T10:38:16.820111Z","shell.execute_reply":"2021-09-29T10:38:17.486851Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# # Define the aggregation procedure outside of the groupby operation\n# aggregations = {\n#     'purchase_amount': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median']\n# }\n\n# grouped = newmerchant_data.groupby('card_id').agg(aggregations)\n# grouped.columns = grouped.columns.droplevel(level=0)\n# grouped.rename(columns={\n#     \"sum\": \"sum_purchase_amount\", \n#     \"mean\": \"mean_purchase_amount\",\n#     \"std\": \"std_purchase_amount\", \n#     \"min\": \"min_purchase_amount\",\n#     \"max\": \"max_purchase_amount\", \n#     \"size\": \"num_purchase_amount\",\n#     \"median\": \"median_purchase_amount\"\n# }, inplace=True)\n# grouped.reset_index(inplace=True)\n\n# grouped.shape\n\n# df_train = pd.merge(df_train, grouped, on=\"card_id\", how=\"left\")\n# df_test = pd.merge(df_test, grouped, on=\"card_id\", how=\"left\")\n\n# del grouped\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Due to card_ids not found in newmerchant_data\n# check_missing_values(df_train), check_missing_values(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Merging with new merchant data","metadata":{}},{"cell_type":"code","source":"aggs = {}\n\n# 'year', 'month', 'weekofyear', 'dayofweek', 'hour', 'merchant_id', 'subsector_id',\nfor col in ['merchant_category_id']:\n            # 'city_id', 'state_id']: # added more features\n    aggs[col] = ['nunique']\n\naggs['purchase_amount'] = ['sum', 'min', 'max', 'mean', 'median'] # ['sum','max','min','mean','var'], 'std'\naggs['installments'] = ['sum', 'max'] # ['sum','max','min','mean','var'], ['std', 'median']\naggs['purchase_date'] = ['min', 'max']\naggs['month_lag'] = ['mean'] # ['max','min','mean','var'], 'std'\naggs['month_diff'] = ['mean']\naggs['weekend'] = ['mean'] # 'sum', \naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\n# include columns with merchant information\n# aggs['merchants_merchant_group_id_nunique'] = ['size']\n# aggs['merchants_subsector_id_nunique'] = ['size']\n# aggs['merchants_category_1_nunique'] = ['size']\n# aggs['merchants_most_recent_sales_range_nunique'] = ['size']\n# aggs['merchants_most_recent_purchases_range_nunique'] = ['size']\n# aggs['merchants_category_4_nunique'] = ['size']\n# aggs['merchants_city_id_nunique'] = ['size']\n# aggs['merchants_state_id_nunique'] = ['size']\n\n# 'new_merchant_merchants_active_months_lag3_mean_max',\n# 'new_merchant_merchants_active_months_lag3_mean_min',\n# 'new_merchant_merchants_active_months_lag3_mean_sum',\n\n# 'new_merchant_merchants_active_months_lag3_sum_min',\n# 'new_merchant_merchants_active_months_lag3_sum_max',\n\n# 'new_merchant_merchants_active_months_lag6_sum_min',\n# 'new_merchant_merchants_active_months_lag6_sum_max',\n# 'new_merchant_merchants_active_months_lag6_mean_min',\n# 'new_merchant_merchants_active_months_lag6_mean_max',\n\n# 'new_merchant_merchants_active_months_lag12_sum_min',\n# 'new_merchant_merchants_active_months_lag12_sum_max',\n# 'new_merchant_merchants_active_months_lag12_mean_max',\n\naggs['merchants_numerical_1_mean'] = ['min', 'max']\naggs['merchants_numerical_2_mean'] = ['min', 'max']\n\n# aggs['avg_sales_lag3'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_avg_sales_lag3_sum'] = ['min', 'max', 'mean']\naggs['merchants_avg_sales_lag3_mean'] = ['min', 'max']\n# aggs['merchants_avg_sales_lag3_size'] = ['min', 'max']\naggs['merchants_avg_sales_lag3_min'] = ['min', 'max']\naggs['merchants_avg_sales_lag3_max'] = ['min', 'max']\n\n# aggs['avg_purchases_lag3'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_avg_purchases_lag3_sum'] = ['min', 'max', 'mean']\naggs['merchants_avg_purchases_lag3_mean'] = ['min', 'max']\n# aggs['merchants_avg_purchases_lag3_size'] = ['min', 'max']\naggs['merchants_avg_purchases_lag3_min'] = ['min', 'max']\naggs['merchants_avg_purchases_lag3_max'] = ['min', 'max']\n\n## aggs['active_months_lag3'] = ['sum', 'mean', 'size', 'min', 'max']\n# aggs['merchants_active_months_lag3_sum'] = ['mean'] # 'min', 'max',\n# aggs['merchants_active_months_lag3_mean'] = ['mean', 'median'] # 'sum', 'min', 'max'\n# aggs['merchants_active_months_lag3_size'] = ['min', 'max']\n# aggs['merchants_active_months_lag3_min'] = ['min', 'max']\n# aggs['merchants_active_months_lag3_max'] = ['min', 'max']\n\n# aggs['avg_sales_lag6'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_avg_sales_lag6_sum'] = ['min', 'max', 'mean']\naggs['merchants_avg_sales_lag6_mean'] = ['min', 'max']\n# aggs['merchants_avg_sales_lag6_size'] = ['min', 'max']\naggs['merchants_avg_sales_lag6_min'] = ['min', 'max']\naggs['merchants_avg_sales_lag6_max'] = ['min', 'max']\n\n# aggs['avg_purchases_lag6'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_avg_purchases_lag6_sum'] = ['min', 'max', 'mean']\naggs['merchants_avg_purchases_lag6_mean'] = ['min'] # 'max'\n# aggs['merchants_avg_purchases_lag6_size'] = ['min', 'max']\naggs['merchants_avg_purchases_lag6_min'] = ['min'] # 'max'\naggs['merchants_avg_purchases_lag6_max'] = ['min', 'max']\n\n# aggs['active_months_lag6'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_active_months_lag6_sum'] = ['mean'] # 'min', 'max'\n# aggs['merchants_active_months_lag6_mean'] = ['min', 'max']\n# aggs['merchants_active_months_lag6_size'] = ['min', 'max']\n# aggs['merchants_active_months_lag6_min'] = ['min', 'max']\n# aggs['merchants_active_months_lag6_max'] = ['min', 'max']\n\n# aggs['avg_sales_lag12'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_avg_sales_lag12_sum'] = ['min', 'max', 'mean']\naggs['merchants_avg_sales_lag12_mean'] = ['min', 'max']\n# aggs['merchants_avg_sales_lag12_size'] = ['min', 'max']\naggs['merchants_avg_sales_lag12_min'] = ['min', 'max']\naggs['merchants_avg_sales_lag12_max'] = ['min', 'max']\n\n# aggs['avg_purchases_lag12'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_avg_purchases_lag12_sum'] = ['min', 'max', 'mean']\naggs['merchants_avg_purchases_lag12_mean'] = ['min', 'max']\n# aggs['merchants_avg_purchases_lag12_size'] = ['min', 'max'] \naggs['merchants_avg_purchases_lag12_min'] = ['min', 'max']\naggs['merchants_avg_purchases_lag12_max'] = ['min', 'max']\n\n# aggs['active_months_lag12'] = ['sum', 'mean', 'size', 'min', 'max']\naggs['merchants_active_months_lag12_sum'] = ['min', 'mean'] #  'max'\naggs['merchants_active_months_lag12_mean'] = ['min', 'max']\n# aggs['merchants_active_months_lag12_size'] = ['min', 'max']\naggs['merchants_active_months_lag12_min'] = ['min', 'max']\n# aggs['merchants_active_months_lag12_max'] = ['min', 'max']\n\naggs['merchants_most_recent_sales_range_min'] = ['min', 'max']\naggs['merchants_most_recent_sales_range_max'] = ['min', 'max']\naggs['merchants_most_recent_purchases_range_min'] = ['min', 'max']\naggs['merchants_most_recent_purchases_range_max'] = ['min', 'max']\n\naggs['merchants_category_1_min'] = ['min', 'max']\naggs['merchants_category_1_max'] = ['min', 'max']\n\n## To be continue for further feature explorations\n##\n\naggs['merchants_numerical_1_sum'] = ['min', 'max', 'mean']\naggs['merchants_numerical_2_sum'] = ['min', 'max', 'mean']\n\naggs['merchants_avg_purchases_lag6_sum'] = ['min', 'max', 'mean']\n\n# aggs['merchants_active_months_lag6_sum'] = ['mean'] # 'min', 'max'\n# aggs['merchants_active_months_lag6_mean'] = ['sum'] #  'min', 'max'\n\naggs['merchants_avg_sales_lag12_sum'] = ['sum', 'min', 'max', 'mean']\naggs['merchants_avg_purchases_lag12_sum'] = ['min', 'max', 'mean']\n\n# aggs['merchants_active_months_lag12_sum'] = ['mean'] # 'min', 'max', \naggs['merchants_active_months_lag12_mean'] = ['sum'] # , 'min', 'max'\n\naggs['merchants_category_2_mean'] = ['sum'] # , 'min', 'max'\n\n# added more features with median for 'installments'\n# aggs['month'] = [ 'min', 'max', 'mean', 'var']\n\nfor col in ['category_2','category_3']:\n    # Replace newmerchant_data with newmerchant_with_merchants_data\n    newmerchant_with_merchants_data[col+'_mean'] = newmerchant_with_merchants_data.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']\n    \nnew_columns = create_new_columns('new_merchant',aggs)\nprint(\"New columns :\\n\", new_columns)\ndf_new_merchant_trans_group = newmerchant_with_merchants_data.groupby('card_id').agg(aggs) # newmerchant_data\n\ndf_new_merchant_trans_group.columns = new_columns\ndf_new_merchant_trans_group.reset_index(drop=False,inplace=True)\ndf_new_merchant_trans_group['new_merchant_purchase_date_diff'] = (df_new_merchant_trans_group['new_merchant_purchase_date_max'] - df_new_merchant_trans_group['new_merchant_purchase_date_min']).dt.days\ndf_new_merchant_trans_group['new_merchant_purchase_date_average'] = df_new_merchant_trans_group['new_merchant_purchase_date_diff'] / df_new_merchant_trans_group['new_merchant_card_id_size']\ndf_new_merchant_trans_group['new_merchant_purchase_date_uptonow'] = (datetime.datetime.today() - df_new_merchant_trans_group['new_merchant_purchase_date_max']).dt.days\n\nprint()\nprint(\"Shape of df_new_merchant_trans_group :\\n\", df_new_merchant_trans_group.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:38:38.482505Z","iopub.execute_input":"2021-09-29T10:38:38.482873Z","iopub.status.idle":"2021-09-29T10:38:47.215290Z","shell.execute_reply.started":"2021-09-29T10:38:38.482844Z","shell.execute_reply":"2021-09-29T10:38:47.214009Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df_new_merchant_trans_group['new_merchant_purchase_date_diff']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:38:47.980839Z","iopub.execute_input":"2021-09-29T10:38:47.981286Z","iopub.status.idle":"2021-09-29T10:38:47.987465Z","shell.execute_reply.started":"2021-09-29T10:38:47.981250Z","shell.execute_reply":"2021-09-29T10:38:47.986506Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Get ready for training models\nX_train = df_train.merge(df_new_merchant_trans_group,on='card_id',how='left')\nX_test = df_test.merge(df_new_merchant_trans_group,on='card_id',how='left')","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:38:51.515005Z","iopub.execute_input":"2021-09-29T10:38:51.515468Z","iopub.status.idle":"2021-09-29T10:38:54.959532Z","shell.execute_reply.started":"2021-09-29T10:38:51.515429Z","shell.execute_reply":"2021-09-29T10:38:54.958131Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# After merging with historical data with 'left' with group by card_id\nprint(df_new_merchant_trans_group.shape, X_train.shape, X_test.shape) # ((201917, 91), (123623, 90))\ndel df_new_merchant_trans_group #, train_data, test_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:38:55.540776Z","iopub.execute_input":"2021-09-29T10:38:55.541236Z","iopub.status.idle":"2021-09-29T10:38:55.927258Z","shell.execute_reply.started":"2021-09-29T10:38:55.541198Z","shell.execute_reply":"2021-09-29T10:38:55.926065Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"X_train['hist_purchase_date_max'].head(2), X_test['hist_purchase_date_max'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:39:10.594062Z","iopub.execute_input":"2021-09-29T10:39:10.594477Z","iopub.status.idle":"2021-09-29T10:39:10.605355Z","shell.execute_reply.started":"2021-09-29T10:39:10.594443Z","shell.execute_reply":"2021-09-29T10:39:10.604197Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"X_train['new_merchant_purchase_date_max'].head(2), X_test['new_merchant_purchase_date_max'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:39:12.067781Z","iopub.execute_input":"2021-09-29T10:39:12.068191Z","iopub.status.idle":"2021-09-29T10:39:12.078097Z","shell.execute_reply.started":"2021-09-29T10:39:12.068157Z","shell.execute_reply":"2021-09-29T10:39:12.076681Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"X_train['hist_purchase_amount_max'].head(2), X_test['hist_purchase_amount_max'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:40:03.162544Z","iopub.execute_input":"2021-09-29T10:40:03.163026Z","iopub.status.idle":"2021-09-29T10:40:03.172194Z","shell.execute_reply.started":"2021-09-29T10:40:03.162968Z","shell.execute_reply":"2021-09-29T10:40:03.170939Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"X_train['new_merchant_purchase_amount_max'].head(2), X_test['new_merchant_purchase_amount_max'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:40:07.560378Z","iopub.execute_input":"2021-09-29T10:40:07.560800Z","iopub.status.idle":"2021-09-29T10:40:07.571226Z","shell.execute_reply.started":"2021-09-29T10:40:07.560767Z","shell.execute_reply":"2021-09-29T10:40:07.570000Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = df_train.merge(df_merchant_category, on='merchant_category_id', how='left')\n# df_test = df_test.merge(df_merchant_category, on='merchant_category_id', how='left')\n\n# df_train.shape, df_test.shape\n# check_missing_values(X_train), check_missing_values(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Concatenate the categorical features","metadata":{}},{"cell_type":"code","source":"# Numerical representations of the nominal feature_1, feature_2, feature_3\nX_train = pd.concat([X_train, ohe_train_df_1, ohe_train_df_2, ohe_train_df_3], axis=1, sort=False)\nX_test = pd.concat([X_test, ohe_test_df_1, ohe_test_df_2, ohe_test_df_3], axis=1, sort=False)\n\ndel ohe_train_df_1, ohe_train_df_2, ohe_train_df_3\ndel ohe_test_df_1, ohe_test_df_2, ohe_test_df_3\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (201917, 101), (123623, 100)\nprint(X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:40:51.209514Z","iopub.execute_input":"2021-09-29T10:40:51.209889Z","iopub.status.idle":"2021-09-29T10:40:51.217227Z","shell.execute_reply.started":"2021-09-29T10:40:51.209853Z","shell.execute_reply":"2021-09-29T10:40:51.215060Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"loyality_score = X_train['target']\nax = loyality_score.plot.hist(bins=20, figsize=(6, 5))\n_ = ax.set_title(\"target histogram\")\nplt.show()\n\nfig, axs = plt.subplots(1,2, figsize=(12, 5))\n_ = loyality_score[loyality_score > 10].plot.hist(ax=axs[0])\n_ = axs[0].set_title(\"target histogram for values greater than 10\")\n_ = loyality_score[loyality_score < -10].plot.hist(ax=axs[1])\n_ = axs[1].set_title(\"target histogram for values less than -10\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are 2207 rows with target values less than -30\nX_train.loc[X_train['target'] < -30, 'target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['outliers'] = 0\n\n# Consider the target values less ta than -30 are outliers\nX_train.loc[X_train['target'] < -30, 'outliers'] = 1\nX_train['outliers'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in ['feature_1','feature_2','feature_3']:\n    # Setting mean value of the 'outliers' for the input features \n    order_label = X_train.groupby([f])['outliers'].mean()\n    X_train[f] = X_train[f].map(order_label)\n    X_test[f] = X_test[f].map(order_label)\n    \nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.loc[0:5, ['feature_1','feature_2','feature_3']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['hist_purchase_date_min'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train[['hist_purchase_date_min', 'hist_purchase_date_max', 'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max']]\n# X_train[['hist_purchase_date_min', 'hist_purchase_date_max', 'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max']].isna().any()\n# X_test[['hist_purchase_date_min', 'hist_purchase_date_max', 'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max']].isna().any()\n\nprint(\"Missing purchase date (min-max) in X_train : \")\nprint(\"min mode : {}\\nmax mode : {}\".format(X_train['new_merchant_purchase_date_min'].mode(),\n                                            X_train['new_merchant_purchase_date_max'].mode()))\nprint(\"-\" * 80)\nprint(\"Missing purchase date (min-max) in X_test : \")\nprint(\"min mode : {}\\nmax mode : {}\".format(X_test['new_merchant_purchase_date_min'].mode(),\n                                            X_test['new_merchant_purchase_date_max'].mode()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X_train['new_merchant_purchase_date_min'].mode()[0]), type(X_train['new_merchant_purchase_date_max'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Purchase data in historical_data\")\nprint(historical_data['purchase_date'].min(), historical_data['purchase_date'].max())\nprint(\"\\nPurchase data in newmerchant_data\")\nprint(newmerchant_data['purchase_date'].min(), newmerchant_data['purchase_date'].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Impute missing purchase min-max values in train and test data","metadata":{}},{"cell_type":"code","source":"# Fill missing values for min-max purchase date newmerchant_data from after merging with newmmerchant_data for the card_ids in train_data\nX_train['new_merchant_purchase_date_min'].fillna(newmerchant_data['purchase_date'].min(), inplace = True)\nX_train['new_merchant_purchase_date_max'].fillna(newmerchant_data['purchase_date'].max(), inplace = True)\nX_train['new_merchant_purchase_amount_min'].fillna(newmerchant_data['purchase_amount'].min(), inplace = True)\nX_train['new_merchant_purchase_amount_max'].fillna(newmerchant_data['purchase_amount'].max(), inplace = True)\n\nX_test['new_merchant_purchase_date_min'].fillna(newmerchant_data['purchase_date'].min(), inplace = True)\nX_test['new_merchant_purchase_date_max'].fillna(newmerchant_data['purchase_date'].max(), inplace = True)\nX_test['new_merchant_purchase_amount_min'].fillna(newmerchant_data['purchase_amount'].min(), inplace = True)\nX_test['new_merchant_purchase_amount_max'].fillna(newmerchant_data['purchase_amount'].max(), inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:45:23.246863Z","iopub.execute_input":"2021-09-29T10:45:23.247340Z","iopub.status.idle":"2021-09-29T10:45:23.330921Z","shell.execute_reply.started":"2021-09-29T10:45:23.247302Z","shell.execute_reply":"2021-09-29T10:45:23.329803Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"X_train[['hist_purchase_date_min', 'hist_purchase_date_max', 'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max',\n         'new_merchant_purchase_amount_min', 'new_merchant_purchase_amount_max']].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:45:27.692768Z","iopub.execute_input":"2021-09-29T10:45:27.693220Z","iopub.status.idle":"2021-09-29T10:45:27.716851Z","shell.execute_reply.started":"2021-09-29T10:45:27.693186Z","shell.execute_reply":"2021-09-29T10:45:27.715609Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"X_test[['hist_purchase_date_min', 'hist_purchase_date_max', 'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max',\n        'new_merchant_purchase_amount_min', 'new_merchant_purchase_amount_max']].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:45:29.941585Z","iopub.execute_input":"2021-09-29T10:45:29.942001Z","iopub.status.idle":"2021-09-29T10:45:29.963919Z","shell.execute_reply.started":"2021-09-29T10:45:29.941967Z","shell.execute_reply":"2021-09-29T10:45:29.962494Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Convert Timestamp to int64 with imputed missing values\nfor f in ['hist_purchase_date_min', 'hist_purchase_date_max', 'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max']:\n    print(type(X_train[f][0]))\n    print(type(X_test[f][0]))\n    X_train[f] = X_train[f].astype(np.int64) * 1e-9\n    X_test[f] = X_test[f].astype(np.int64) * 1e-9","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:45:43.967438Z","iopub.execute_input":"2021-09-29T10:45:43.967888Z","iopub.status.idle":"2021-09-29T10:45:44.015779Z","shell.execute_reply.started":"2021-09-29T10:45:43.967853Z","shell.execute_reply":"2021-09-29T10:45:44.015031Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:45:47.380478Z","iopub.execute_input":"2021-09-29T10:45:47.380851Z","iopub.status.idle":"2021-09-29T10:45:47.388246Z","shell.execute_reply.started":"2021-09-29T10:45:47.380820Z","shell.execute_reply":"2021-09-29T10:45:47.387224Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# This strong ration feature elevates the ranking by 10%+\nX_train['new_hist_purchase_date_max_ratio'] = X_train['new_merchant_purchase_date_max'] / X_train['hist_purchase_date_max']\nX_test['new_hist_purchase_date_max_ratio'] = X_test['new_merchant_purchase_date_max'] / X_test['hist_purchase_date_max']\n\n# Need to try if the ratio improves the overall RMSE\nX_train['new_hist_purchase_amount_max_ratio'] = X_train['new_merchant_purchase_amount_max'] / X_train['hist_purchase_amount_max']\nX_test['new_hist_purchase_amount_max_ratio'] = X_test['new_merchant_purchase_amount_max'] / X_test['hist_purchase_amount_max']\n\n# Need to produce 'card_purchase_date_max_ratio'\n\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:46:26.427012Z","iopub.execute_input":"2021-09-29T10:46:26.427473Z","iopub.status.idle":"2021-09-29T10:46:26.446614Z","shell.execute_reply.started":"2021-09-29T10:46:26.427437Z","shell.execute_reply":"2021-09-29T10:46:26.445554Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# for f in ['new_merchant_purchase_date_max',\n#           'hist_purchase_date_max',]:\n#     X_train[f +'_int'] = X_train[f].astype(np.int64) * 1e-9\n#     X_test[f +'_int'] = X_test[f].astype(np.int64) * 1e-9\n    \n# X_train['card_purchase_date_max_ratio'] = X_train['new_merchant_purchase_date_max_int'] / X_train['hist_purchase_date_max_int']\n# X_test['card_purchase_date_max_ratio'] = X_test['new_merchant_purchase_date_max_int'] / X_test['hist_purchase_date_max_int']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['new_merchant_merchants_category_1_min_min'].fillna(newmerchant_data['category_1'].min(), inplace = True)\nX_train['new_merchant_merchants_category_1_min_max'].fillna(newmerchant_data['category_1'].min(), inplace = True)\nX_train['new_merchant_merchants_category_1_max_min'].fillna(newmerchant_data['category_1'].max(), inplace = True)\nX_train['new_merchant_merchants_category_1_max_max'].fillna(newmerchant_data['category_1'].max(), inplace = True)\n\nX_test['new_merchant_merchants_category_1_min_min'].fillna(newmerchant_data['category_1'].min(), inplace = True)\nX_test['new_merchant_merchants_category_1_min_max'].fillna(newmerchant_data['category_1'].min(), inplace = True)\nX_test['new_merchant_merchants_category_1_max_min'].fillna(newmerchant_data['category_1'].max(), inplace = True)\nX_test['new_merchant_merchants_category_1_max_max'].fillna(newmerchant_data['category_1'].max(), inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:52:34.668767Z","iopub.execute_input":"2021-09-29T10:52:34.669159Z","iopub.status.idle":"2021-09-29T10:52:34.708758Z","shell.execute_reply.started":"2021-09-29T10:52:34.669130Z","shell.execute_reply":"2021-09-29T10:52:34.707459Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"X_train[['new_merchant_merchants_category_1_min_min', 'new_merchant_merchants_category_1_min_max',\n         'new_merchant_merchants_category_1_max_min', 'new_merchant_merchants_category_1_max_max']].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:52:41.711771Z","iopub.execute_input":"2021-09-29T10:52:41.712300Z","iopub.status.idle":"2021-09-29T10:52:41.733211Z","shell.execute_reply.started":"2021-09-29T10:52:41.712266Z","shell.execute_reply":"2021-09-29T10:52:41.731870Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# excluded_features = ['first_active_month', 'card_id', 'target', 'date', 'year']\n# Excluse non numeric features\n# excluded_features = ['first_active_month', 'card_id', 'target', 'hist_purchase_date_min', 'hist_purchase_date_max']\n# train_features = [c for c in df_train.columns if c not in excluded_features]\n\n# 'hist_purchase_date_min', 'hist_purchase_date_max', 'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max'\n# Consider removal of these 'new_merchant_purchase_date_max', 'hist_purchase_date_max' after getting their ratio\nexcluded_features = ['first_active_month', 'card_id', 'target', 'outliers']\ntrain_features = [c for c in X_train.columns if c not in excluded_features]\n\nprint(\"Features used for training : \", train_features)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:54:34.505109Z","iopub.execute_input":"2021-09-29T10:54:34.505595Z","iopub.status.idle":"2021-09-29T10:54:34.512849Z","shell.execute_reply.started":"2021-09-29T10:54:34.505559Z","shell.execute_reply":"2021-09-29T10:54:34.511941Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Filled mean information for the missing cards from newmerchant_data\n# Fill missing values with mean values; maybe use median value\nfor col in train_features:\n    for df in [X_train, X_test]:\n        if df[col].dtype == \"float64\":\n            df[col] = df[col].fillna(df[col].median()) # mean","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-29T10:54:39.562070Z","iopub.execute_input":"2021-09-29T10:54:39.562608Z","iopub.status.idle":"2021-09-29T10:54:40.733695Z","shell.execute_reply.started":"2021-09-29T10:54:39.562573Z","shell.execute_reply":"2021-09-29T10:54:40.732764Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# X_train['new_merchant_merchants_category_1_min_min'].mode()[0]\n\n# # Fill missing values for min-max purchase date newmerchant_data from after merging with newmmerchant_data for the card_ids in train_data\n# X_train['new_merchant_merchants_category_1_min_min'].fillna(newmerchant_data['category_1'].min(), inplace = True)\n# X_train['new_merchant_merchants_category_1_min_max'].fillna(newmerchant_data['category_1'].min(), inplace = True)\n# X_train['new_merchant_merchants_category_1_max_min'].fillna(newmerchant_data['category_1'].max(), inplace = True)\n# X_train['new_merchant_merchants_category_1_max_max'].fillna(newmerchant_data['category_1'].max(), inplace = True)\n\n# X_test['new_merchant_merchants_category_1_min_min'].fillna(newmerchant_data['category_1'].min(), inplace = True)\n# X_test['new_merchant_merchants_category_1_min_max'].fillna(newmerchant_data['category_1'].min(), inplace = True)\n# X_test['new_merchant_merchants_category_1_max_min'].fillna(newmerchant_data['category_1'].max(), inplace = True)\n# X_test['new_merchant_merchants_category_1_max_max'].fillna(newmerchant_data['category_1'].max(), inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(check_missing_values(X_train)), len(check_missing_values(X_test))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-29T10:54:43.624155Z","iopub.execute_input":"2021-09-29T10:54:43.624728Z","iopub.status.idle":"2021-09-29T10:54:43.790204Z","shell.execute_reply.started":"2021-09-29T10:54:43.624675Z","shell.execute_reply":"2021-09-29T10:54:43.789466Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"X_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding Correlation between variables of newmerchant_data features\nselected_columns = ['category_2', 'month_lag', 'purchase_amount', 'state_id', 'subsector_id', 'installments']\ndata_frame = newmerchant_data[selected_columns]\n\ndata_frame = data_frame.dropna()\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(data_frame.iloc[:,:].values, i) for i in range(data_frame.shape[1])]\nvif[\"features\"] = data_frame.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data['category_3'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data['authorized_flag'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dict = {'A':1,'B':2,'C':3}\n# value_counts {'Y':1,'N':0}\n\nselected_columns = ['category_2', 'month_lag', 'purchase_amount', 'state_id',\n                    'subsector_id', 'installments', 'authorized_flag', 'month_lag', 'category_3'] \ndata_frame = newmerchant_data[selected_columns]\ndata_frame['category_3'] = data_frame['category_3'].map(Dict)\n# data_frame['authorized_flag'] = data_frame['authorized_flag'].map(Dict1)\n\ndata_frame = data_frame.dropna()\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(data_frame.iloc[:,:].values, i) for i in range(data_frame.shape[1])]\nvif[\"features\"] = data_frame.columns\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Code snippets","metadata":{}},{"cell_type":"code","source":"# Aggregate works only for numeric colums\n# df_new_merchant_trans_group = temp_df.groupby('card_id').agg(['mean', 'median'])\n# df_new_merchant_trans_group.columns = ['1', '2', '3', '4',\n#                                        '5', '6', '7', '8',\n#                                        '9', '10', '11', '12',\n#                                        '13', '14', '15', '16', '17']\n# df_new_merchant_trans_group.reset_index(drop=False,inplace=True)\n# df_new_merchant_trans_group\n\n# df_new_merchant_trans_group = temp_df.groupby('card_id')\n# df_new_merchant_trans_group.obj.columns\n\n# a = temp_df.groupby('card_id').count()\n# a.reset_index(drop=False,inplace=True)\n\n# newmerchant_data.loc[newmerchant_data['card_id']=='C_ID_415bb3a509']\n\n# df_new_merchant_trans_group = merchant_test_df.groupby('card_id').agg(['mean', 'median'])\n# df_new_merchant_trans_group.columns = ['1', '2', '3', '4',\n#                                        '5', '6', '7', '8',\n#                                        '9', '10', '11', '12',\n#                                        '13', '14', '15', '16', '17']\n# df_new_merchant_trans_group.reset_index(drop=False,inplace=True)\n# df_new_merchant_trans_group","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train.iloc[df_train['new_merchant_purchase_amount_sum'].isna().any()]\n# missing_card_id = test_data.loc[test_data['first_active_month'].isna(), 'card_id'].reset_index(drop=True)[0]\n# # get the historical data for the missing card_id\n# card_missing_first_active_month = historical_data.loc[historical_data['card_id'] == missing_card_id]\n\n# print(\"Card_id : {} with {} transactions.\".format(missing_card_id,\n#                                                   card_missing_first_active_month.shape[0]))\n\nmissing_sum_card_id = X_train.loc[X_train['new_merchant_purchase_amount_sum'].isna(), 'card_id'].reset_index(drop=True)[0]\n\nprint(\"Missing sum for card_id: \", missing_sum_card_id)\n# df_train.loc[df_train['new_merchant_purchase_amount_sum'].isna().any(), 'new_merchant_purchase_amount_sum']\ncard_missing_sum = X_train.loc[X_train['card_id'] == missing_sum_card_id]\ncard_missing_sum.iloc[:, 28:29]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are card_id in train_data but not in newmerchant_data\nnewmerchant_data.loc[newmerchant_data['card_id'] == missing_sum_card_id]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.loc[df_train['card_id'] == missing_sum_card_id]\n\n# train_data.loc[train_data['card_id'] == missing_sum_card_id]\n# test_data.loc[test_data['card_id'] == missing_sum_card_id]\n# historical_data.loc[historical_data['card_id'] == missing_sum_card_id]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_new_merchant_trans_group.shape, df_train.shape, df_test.shape)\ndel df_new_merchant_trans_group; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# More Feature Engineering\nfor df in [df_train, df_test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max', 'new_hist_purchase_date_min']:\n        df[f] = df[f].fillna(datetime.datetime.today(),inplace=True)# df[f].astype(np.int64) * 1e-9\n    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n\nfor f in ['feature_1','feature_2','feature_3']:\n    order_label = df_train.groupby([f])['outliers'].mean()\n    train_data[f] = df_train[f].map(order_label)\n    test_data[f] = df_test[f].map(order_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are duplicate card_ids in historical_data with 29,112,361 transactions, unique values are 325,540\n# There are missing values in category_3', 'merchant_id' and 'category_2'\n\n# Replace missing values with mode values for category_3', 'merchant_id' and 'category_2'\n# When inplace = True, the data is modified in place,\n# which means it will return nothing and the dataframe is now updated.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Need to impute values which is the challenge for prediction\n\n# Stable documentation : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html#pandas.merge\n# Merge train_data[['card_id','target']] with historical_data to give new historical_data\n# merging target value of card_id for each transaction in historical_transactions Data; using union of keys from both frames\n# historical_data = pd.merge(historical_data, train_data[['card_id','target']], how = 'outer', on = 'card_id')\n\n# # merging target value of card_id for each transaction in new_merchants_transactions Data\n# newmerchant_data = pd.merge(newmerchant_data, train_data[['card_id','target']], how = 'outer', on = 'card_id')\n\n# historical_data['target'].isnull().sum() # / historical_data.shape[0] = 38.07%\n# historical_data['target'].describe()\n\n# for row in range(historical_data.shape[0]):\n#   if (historical_data['card_id'][row] == test_data['card_id'][10]):\n#     print(row)\n\n# for card_id[0], 'C_ID_0ab67a22ab' : there are 1304310 - 1304243 = 67 historical transactions\n# for card_id[10], 'C_ID_4859ac9ed5' : there are 23622204 - 23622180 = 24 historical transactions\n# historical_data.loc[[1304243]]\n\n# reset_index() to keep the 'card_id' column that appears with X.columns\n# X = partial_historical_data_target.groupby('card_id').mean().reset_index()\n\n# groupby_card_id.isnull().sum()\n\n# Merge test_data[['card_id']] with partial_historical_data without target to give new test_data_intersect\n# test_data_intersect = pd.merge(historical_data, test_data[['card_id']], how = 'inner', on = 'card_id')\n# test_data_intersect\n\n# Drop the non-numeric features\n# partial_historical_data = historical_data_target.drop(columns=['target', 'authorized_flag', 'category_1', 'category_3', 'merchant_id', 'purchase_date'])\n\n# historical_data['target'] = historical_data['target'].replace(0, historical_data['target'].mode())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Baseline Model :</h2>","metadata":{"id":"ABWGki1H6vJM"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Cost : Root Mean Square Error, RMSE\nfrom sklearn.metrics import mean_squared_error\n# Better Evaluation using cross-validation\nfrom sklearn.model_selection import cross_val_score\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard Deviation:\", scores.std())\n\n# Linear Regression model\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train, data_labels)\n\n# first_batchdata = X_train.iloc[:5]\n# first_batchlabels = data_labels.iloc[:5]\n# print(\"Predictions: \", linear_reg.predict(first_batchdata))\n# print(\"Labels: \", list(first_batchlabels))\n\ndata_predictions = linear_reg.predict(X_train)\nlinear_mse = mean_squared_error(data_labels, data_predictions)\nlinear_rmse = np.sqrt(linear_mse)\n\nlinear_rmse # underfitting data; high bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# More Powerful Algorithm : DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(X_train[train_features], X_train['target'])\n\ntree_reg_predictions = tree_reg.predict(X_train[train_features])\ntree_mse = mean_squared_error(X_train['target'], tree_reg_predictions)\ntree_rmse = np.sqrt(tree_mse)\n\nprint(\"DecisionTreeRegressor RMSE :\", tree_rmse)\n\n# scores = cross_val_score(tree_reg, X_train, data_labels,\n#                          scoring=\"neg_mean_squared_error\", cv=10)\n\n# tree_rmse_scores = np.sqrt(-scores) # opposite of MSE, need to have negative sign\n\n# # Different results when it is executed\n# display_scores(tree_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[train_features].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"card_ids = X_test[\"card_id\"].copy()\ntree_reg_predictions = tree_reg.predict(X_test[train_features])\n\ncreate_file_for_submission(\"dtreereg_85_feats.csv\", card_ids, tree_reg_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Ensemble Learning : RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22\nforest_reg = RandomForestRegressor(n_estimators=10)\nforest_reg.fit(X_train[train_features], y)\n\ndata_predictions = forest_reg.predict(X_train[train_features])\nforest_mse = mean_squared_error(y, data_predictions)\nforest_rmse = np.sqrt(forest_mse)\n\nprint(\"RandomForestRegressor RMSE :\", forest_rmse)\n\n# forest_scores = cross_val_score(forest_reg, X_train, data_labels,\n#                                 scoring=\"neg_mean_squared_error\", cv=10)\n\n# forest_rmse_scores = np.sqrt(-forest_scores) # opposite of MSE, need to have negative sign\n\n# display_scores(forest_rmse_scores)\n\n# Scores: [3.90089189 4.04644846 3.99770616 3.98337336 4.00308421 3.89694652\n#  3.9277469  4.12725579 4.00782351 4.04242215]\n# Mean: 3.993369895502748\n# Standard Deviation: 0.06762168624862287\n# CPU times: user 14min 51s, sys: 1.45 s, total: 14min 52s\n# Wall time: 14min 53s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Search the best combination of hyperparameter values\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [10, 30], 'max_features': [10, 15, 20]}, # 'max_features': [2, 4, 6, 8]\n    {'bootstrap': [False], 'n_estimators': [5, 15], 'max_features': [10, 20]}, # 'max_features': [2, 3, 4]\n]\n\n# The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22\nforest_reg = RandomForestRegressor(n_estimators=10) # no more warning on default 'n_estimators' value\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=\"neg_mean_squared_error\")\n\ngrid_search.fit(X_train[train_features], y)\n\n# CPU times: user 46min 45s, sys: 4.79 s, total: 46min 50s\n# Wall time: 46min 50s\n\n# GridSearchCV(cv=5, estimator=RandomForestRegressor(n_estimators=10),\n#              param_grid=[{'max_features': [10, 15, 20],\n#                           'n_estimators': [3, 10, 30]},\n#                          {'bootstrap': [False], 'max_features': [9, 11, 17],\n#                           'n_estimators': [3, 10]}],\n#              scoring='neg_mean_squared_error')\n\n# grid_search.best_params_\n# {'max_features': 15, 'n_estimators': 30}\n\n# GridSearchCV(cv=5, estimator=RandomForestRegressor(n_estimators=10),\n#              param_grid=[{'max_features': [10, 15, 20, 30, 37],\n#                           'n_estimators': [3, 10, 30]},\n#                          {'bootstrap': [False],\n#                           'max_features': [9, 11, 17, 27, 31],\n#                           'n_estimators': [3, 10]}],\n#              scoring='neg_mean_squared_error')\n\n# grid_search.best_params_\n# {'max_features': 10, 'n_estimators': 30}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_result = grid_search.cv_results_\nfor mean_score, params in zip(cv_result[\"mean_test_score\"], cv_result[\"params\"]):\n    print(np.sqrt(-mean_score), params) # negative mean_score\n    \n# RMSE : 3.745822393200406 {'max_features': 10, 'n_estimators': 30} with 85 features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyze the Best Models and their Errors\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(feature_importances)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using GridSearchCV to obtain final model with best estimator\n# final_model = grid_search.best_estimator_\nrf_best = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_predictions = rf_best.predict(X_test[train_features])\nrf_predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[train_features].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_file_for_submission(\"rf_85_feats.csv\", card_ids, rf_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Model\n\n> https://xgboost.readthedocs.io/en/latest/parameter.html#","metadata":{}},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:56:36.444081Z","iopub.execute_input":"2021-09-29T10:56:36.444499Z","iopub.status.idle":"2021-09-29T10:56:36.451259Z","shell.execute_reply.started":"2021-09-29T10:56:36.444468Z","shell.execute_reply":"2021-09-29T10:56:36.450152Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Check for missing values : X_train.isna().any()\ncheck_missing_values(X_train), check_missing_values(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:56:39.343031Z","iopub.execute_input":"2021-09-29T10:56:39.343421Z","iopub.status.idle":"2021-09-29T10:56:39.500398Z","shell.execute_reply.started":"2021-09-29T10:56:39.343385Z","shell.execute_reply":"2021-09-29T10:56:39.499246Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"X_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_test['new_merchant_purchase_date_min'].describe()\n# X_test['new_merchant_purchase_date_min'].isna().value_counts()\n# X_test['card_id'].isna().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for intersection between train features and missing values\nintersect_features = set(train_features).intersection(check_missing_values(X_test)) # df_train\nprint(\"Common features in train and missing value test features (should be 0): {}\".format(len(intersect_features)))\nintersect_features","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:56:43.397219Z","iopub.execute_input":"2021-09-29T10:56:43.397620Z","iopub.status.idle":"2021-09-29T10:56:43.467171Z","shell.execute_reply.started":"2021-09-29T10:56:43.397586Z","shell.execute_reply":"2021-09-29T10:56:43.466089Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\nnp.random.seed(2728)\n\n# Prepare data for training and prediction\nX = X_train.copy()\ny = X['target']\ncard_ids = X_test[\"card_id\"].copy()\n\nkfolds = KFold(n_splits=17, shuffle=True, random_state=2018)\n\nprint(\"kfolds :\", kfolds)\n# Make importance dataframe\nimportances = pd.DataFrame()\n\noof_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(X_test.shape[0])\n\nX.shape, y.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:56:50.166566Z","iopub.execute_input":"2021-09-29T10:56:50.166930Z","iopub.status.idle":"2021-09-29T10:56:50.526087Z","shell.execute_reply.started":"2021-09-29T10:56:50.166897Z","shell.execute_reply":"2021-09-29T10:56:50.524918Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Difference in card_ids\ncol_difference = set(X.columns).symmetric_difference(X_test.columns)\nprint(\"Difference in train and test features: {}\".format(len(col_difference)))\ncol_difference","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:56:51.165435Z","iopub.execute_input":"2021-09-29T10:56:51.165799Z","iopub.status.idle":"2021-09-29T10:56:51.175367Z","shell.execute_reply.started":"2021-09-29T10:56:51.165768Z","shell.execute_reply":"2021-09-29T10:56:51.173920Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"np.isinf(X[train_features]).any().value_counts()\n\n# print(\"printing column name where infinity is present\")\n# col_name = X[train_features].columns.to_series()[np.isinf(X[train_features]).any()]\n# print(col_name)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:56:54.543668Z","iopub.execute_input":"2021-09-29T10:56:54.544098Z","iopub.status.idle":"2021-09-29T10:56:54.827319Z","shell.execute_reply.started":"2021-09-29T10:56:54.544058Z","shell.execute_reply":"2021-09-29T10:56:54.826619Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"np.isinf(X_test[train_features]).any().value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:56:57.098541Z","iopub.execute_input":"2021-09-29T10:56:57.098930Z","iopub.status.idle":"2021-09-29T10:56:57.303601Z","shell.execute_reply.started":"2021-09-29T10:56:57.098895Z","shell.execute_reply":"2021-09-29T10:56:57.302489Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape, df_test.shape)\ndel df_train, df_test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n## timeit will cause problem with importance\nfor n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X[train_features], y)): # X\n    X_train_kf, y_train_kf = X[train_features].iloc[trn_idx], y.iloc[trn_idx]\n    X_valid_kf, y_valid_kf = X[train_features].iloc[val_idx], y.iloc[val_idx]\n    \n    print(\"XGBR fold :\", n_fold)\n    print(\"=\" * 80)\n    \n    # XGBoost Regressor estimator\n    xgb_model = xgb.XGBRegressor(\n        max_depth = 15,\n        learning_rate = 0.01,\n        n_estimators = 1000,\n        subsample = .9,\n        colsample_bylevel = .9,\n        colsample_bytree = .9,\n        min_child_weight= .9,\n        gamma = 0,\n        random_state = 100,\n        booster = 'gbtree',\n        objective = 'reg:squarederror' # 'reg:linear' deprecated\n    )\n    \n    # Training\n    xgb_model.fit(\n        X_train_kf, y_train_kf,\n        eval_set=[(X_train_kf, y_train_kf), (X_valid_kf, y_valid_kf)],\n        verbose=True, eval_metric='rmse',\n        early_stopping_rounds=100\n    )\n    \n    # Feature importance\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = xgb_model.feature_importances_\n    imp_df['fold'] = n_fold + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_preds[val_idx] = xgb_model.predict(X_valid_kf)\n    test_preds = xgb_model.predict(X_test[train_features])\n    sub_preds += test_preds / kfolds.n_splits\n    \n    print(\"Next fold :\", n_fold+1)\n    print()\n    \nprint(\"Final RMSE : \", np.sqrt(mean_squared_error(y, oof_preds)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Validations with RMSE\n>[324]\tvalidation_0-rmse:1.78531\tvalidation_1-rmse:3.75632</br>\n> RMSE : 3.7005642695081877\n\n>[334]\tvalidation_0-rmse:1.75306\tvalidation_1-rmse:3.75425</br>\n> RMSE : 3.702697181424846\n\n> [352]\tvalidation_0-rmse:1.71289\tvalidation_1-rmse:3.75283</br>\n> RMSE : 3.7000391746720824</br>\n\n> [357]\tvalidation_0-rmse:1.64156\tvalidation_1-rmse:3.74462</br>\n> RMSE : 3.6941329565387844</br>\n\n> [378]\tvalidation_0-rmse:1.54589\tvalidation_1-rmse:3.73675</br>\n> Final RMSE :  3.6836558989291652\n\n> [391]\tvalidation_0-rmse:1.51994\tvalidation_1-rmse:3.73630</br>\n> Final RMSE :  3.6854681990466274</br>\n\n> [385]\tvalidation_0-rmse:1.61745\tvalidation_1-rmse:3.73726</br>\n> Final RMSE :  3.687870975295424</br>\n> Wall time: 1h 19min 11s\n\n> [645]\tvalidation_0-rmse:1.31063\tvalidation_1-rmse:3.90872</br>\n> Kfold : 17\n> Final RMSE :  3.679190047236234\n> Wall time: 5h 28min 14s","metadata":{}},{"cell_type":"markdown","source":"#### Save trained ML models","metadata":{}},{"cell_type":"code","source":"# Pickling files\nprint(\"Pickling one of the XGBRegressor model...\")\nfilename = './XGBRegressor_model_pickle'\nsave_model_to_picklefile(filename, xgb_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### If XGBRegressor model exists, load it¶\nretrieve_xgbr_file = './XGBRegressor_model_pickle'\n\nif os.path.exists(retrieve_xgbr_file):\n    # xgb_Classifier.save_model and load_model give an \"le\" error when trying to obtain score\n    # Unpickling saved binary file if exist so that training do not need to done\n    loaded_XGBRegressor = load_model_from_picklefile(retrieve_xgbr_file)\n    print(\"Unpickling existing XGBRegressor model...\")\n    print(\"Loaded model :\\n\", loaded_XGBRegressor)\n    print(\"with type\\n\", type(loaded_XGBRegressor))\n    \n\nxgbr_predictions = loaded_XGBRegressor.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importances","metadata":{}},{"cell_type":"code","source":"importances['gain_log'] = importances['gain']\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(16, 24))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))\n\nplt.title('XGBRegressor Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('xgbr_importances.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Length of submission\nlen(sub_preds), n_fold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr_predictions = sub_preds\nxgbr_predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[train_features].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_file_for_submission(\"xgbr_136_feats_KFold17.csv\", card_ids, xgbr_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final model predictions for submission","metadata":{}},{"cell_type":"code","source":"# len(feature_importance_df)\n# feature_importance_df[[\"feature\", \"importance\"]].sort_values(by=\"importance\", ascending=False)[570:580]\n\n# remove featuress to get better CV RMSE scores\n# print(\"No. of train_features :\", len(train_features))\n# features_to_remove = ['new_merchant_card_id_size', 'new_merchant_merchants_active_months_lag12_mean_min',\n#                       'new_merchant_merchants_active_months_lag3_sum_mean', 'new_merchant_merchants_active_months_lag6_sum_mean',\n#                       'new_merchant_merchants_active_months_lag6_mean_sum', 'new_merchant_merchant_id_nunique',\n#                       'new_merchant_year_nunique', 'new_merchant_weekend_sum',\n#                       'hist_dayofweek_nunique', 'new_merchant_merchants_category_2_mean_max',\n#                       'new_merchant_dayofweek_nunique', 'new_merchant_merchants_category_2_mean_min']\n\n# train_features = list(set(train_features) - set(features_to_remove))\n\n# print(\"No. of train_features after removing some low important features :\", len(train_features))\n\n# len(set(features_to_remove)), len(set(train_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_features\n\n# remove featuress to get better CV RMSE scores\n# print(\"No. of train_features :\", len(train_features))\n# features_to_remove = ['f1_1', 'f1_2', 'f1_3', 'f1_4', 'f1_5',\n#                       'f2_1', 'f2_2', 'f2_3', 'f3_0', 'f3_1']\n\n# train_features = list(set(train_features) - set(features_to_remove))\n\n# print(\"No. of train_features after removing some low important features :\", len(train_features))\n\n# len(set(features_to_remove)), len(set(train_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\n# import warnings\n# warnings.filterwarnings('ignore')\nnp.random.seed(4590)\n    \nlgbm_params = {'num_leaves': 111,\n               'min_data_in_leaf': 149,\n               'objective':'regression',\n               'max_depth': 9,\n               'learning_rate': 0.005,\n               \"boosting\": \"gbdt\",\n               \"feature_fraction\": 0.7522,\n               \"bagging_freq\": 1,\n               \"bagging_fraction\": 0.7083,\n               \"bagging_seed\": 11,\n               \"metric\": 'rmse',\n               \"lambda_l1\": 0.2634,\n               \"random_state\": 133,\n               \"verbosity\": -1}","metadata":{"execution":{"iopub.status.busy":"2021-09-29T10:57:13.641793Z","iopub.execute_input":"2021-09-29T10:57:13.642138Z","iopub.status.idle":"2021-09-29T10:57:14.837345Z","shell.execute_reply.started":"2021-09-29T10:57:13.642109Z","shell.execute_reply":"2021-09-29T10:57:14.836227Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# lgbmreg_param = {'num_leaves': 111,\n#                  'min_child_samples': 149,\n#                  'max_depth': 9,\n#                  'learning_rate': 0.005,\n#                  \"boosting_type\": \"gbdt\",\n#                  \"reg_lambda\": 0.2634,\n#                  \"random_state\": 133,\n#                  \"verbosity\": -1}\n\n# lgb.LGBMRegressor.fit(X[train_features], y,\n#                       num_round,\n#                       eval_sets = [trn_data, val_data],\n#                       verbose=100,\n#                       early_stopping_rounds = 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport time\ncategorical_feats = ['feature_1', 'feature_2', 'feature_3'] # 'feature_1', \nnum_round = 10000\n\nfolds = KFold(n_splits=17, shuffle=True, random_state=15)\noof = np.zeros(len(X[train_features]))\nlgbm_predictions = np.zeros(len(X_test[train_features]))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X[train_features].values, y.values)):\n    print(\"LGBM fold n°{}\".format(fold_))\n    print(\"-\" * 80)\n    \n    trn_data = lgb.Dataset(X.iloc[trn_idx][train_features],\n                           label=y.iloc[trn_idx],\n                           categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(X.iloc[val_idx][train_features],\n                           label=y.iloc[val_idx],\n                           categorical_feature=categorical_feats)\n\n    # num_round = 10000\n    lgbm_reg = lgb.train(lgbm_params,\n                         trn_data,\n                         num_round,\n                         valid_sets = [trn_data, val_data],\n                         verbose_eval=100,\n                         early_stopping_rounds = 200)\n    \n    oof[val_idx] = lgbm_reg.predict(X.iloc[val_idx][train_features], num_iteration=lgbm_reg.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = train_features\n    fold_importance_df[\"importance\"] = lgbm_reg.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    lgbm_predictions += lgbm_reg.predict(X_test[train_features], num_iteration=lgbm_reg.best_iteration) / folds.n_splits\n    print()\n\nprint(\"CV RMSE score: {:<8.5f}\".format(mean_squared_error(oof, y)**0.5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_predictions.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:05:42.890314Z","iopub.execute_input":"2021-09-29T12:05:42.890683Z","iopub.status.idle":"2021-09-29T12:05:42.897031Z","shell.execute_reply.started":"2021-09-29T12:05:42.890647Z","shell.execute_reply":"2021-09-29T12:05:42.895919Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"X[train_features].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:05:42.899212Z","iopub.execute_input":"2021-09-29T12:05:42.899947Z","iopub.status.idle":"2021-09-29T12:05:42.978176Z","shell.execute_reply.started":"2021-09-29T12:05:42.899891Z","shell.execute_reply":"2021-09-29T12:05:42.977429Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"create_file_for_submission(\"lgbm_142_feats_merid_pdmax_median_KFold17.csv\", card_ids, lgbm_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:06:15.627682Z","iopub.execute_input":"2021-09-29T12:06:15.628391Z","iopub.status.idle":"2021-09-29T12:06:16.163240Z","shell.execute_reply.started":"2021-09-29T12:06:15.628318Z","shell.execute_reply":"2021-09-29T12:06:16.161688Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(r'lgbm_85_feats_catid_median_KFold17.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Download for submission :\n> RandomForestRegressor with GridSearchCV using 3 features</br>\n> Rank : 3719 out of 4110 [Private Score : 3.81301]</br>\n> Rank : 3749 out of 4110 [Public Score : 3.93004]\n\n> RandomForestRegressor(max_features=2, n_estimators=30) with some historical features</br>\n> Rank : 3416 out of 4110 [Private Score : 3.79375]</br>\n> Rank : 3238 out of 4110 [Public Score : 3.87633]\n\n> XGBRegressor with 23 historical features</br>\n> Rank : 3115 out of 4110 [Private Score : 3.73174] (75.7%)</br>\n> Rank : 3104 out of 4110 [Public Score : 3.82532] (75.5%)\n\n> XGBRegressor with 34 historical and new merchant features with mean_squared_error of 3.724</br>\n> validation_0-rmse:1.94363\tvalidation_1-rmse:3.77652</br>\n> Rank : 3027 out of 4110 [Private Score : 3.70444] (73.65%)</br>\n> Rank : 3016 out of 4110 [Public Score : 3.79859] (73.38%)\n\n> XGBRegressor with 45 historical and new merchant features with RMSE of 3.7006.</br>\n> validation_0-rmse:1.78531\tvalidation_1-rmse:3.75632</br>\n> Rank : 2757 out of 4110 [Private Score : 3.65051] (67.08%)</br>\n> Rank : 2824 out of 4110 [Public Score : 3.74154] (68.71%)\n\n> XGBRegressor with 55 historical (10 OHE for features1-3) and new merchant features with RMSE of 3.7027.</br>\n> validation_0-rmse:1.75306\tvalidation_1-rmse:3.75425</br>\n> Rank : 2766 out of 4110 [Private Score : 3.65203] (67.30%)</br>\n> Rank : 2816 out of 4110 [Public Score : 3.74019] (68.52%)\n\n> XGBRegressor with 59 historical and new merchant features with RMSE of 3.6837.</br>\n> validation_0-rmse:1.54589\tvalidation_1-rmse:3.73675</br>\n> Rank : 2755 out of 4110 [Private Score : 3.65021] (67.03%)</br>\n> Rank : 2818 out of 4110 [Public Score : 3.74044] (68.56%)\n\n> XGBRegressor with 69 historical and new merchant features with RMSE of 3.6837.</br>\n> validation_0-rmse:1.60083\tvalidation_1-rmse:3.73686</br>\n> Final RMSE :  3.686220904191716</br>\n> Rank : 2756 out of 4110 [Private Score : 3.65033] (67.06%)</br>\n> Rank : 2781 out of 4110 [Public Score : 3.73777] (67.66%)\n\n> XGBRegressor with 79 historical (10 OHE for features1-3) and new merchant features with RMSE of 3.7027.</br>\n> validation_0-rmse:1.51994\tvalidation_1-rmse:3.73630</br>\n> Final RMSE :  3.6854681990466274</br>\n> Rank : 2747 out of 4110 [Private Score : 3.64916] (66.84%)</br>\n> Rank : 2756 out of 4110 [Public Score : 3.73545] (67.06%)</br>\n\n> LGBM with 79 historical (10 OHE for features1-3) and new merchant features with RMSE of 3.6545.</br>\n> [1041] training's rmse: 3.49544\tvalid_1's rmse: 3.60891\n> CV score: 3.65450 </br>\n> Rank : 2115 out of 4110 [Private Score : 3.62122] (51.46%)</br>\n> Rank : 2370 out of 4110 [Public Score : 3.70550] (57.66%)</br>\n\n> LGBM with 83 (10 OHE for features1-3 and new merchant purchase date min max mode) features at RMSE of 3.65311.</br>\n> [1120] training's rmse: 3.4844\tvalid_1's rmse: 3.6065\n> CV score: 3.65311 </br>\n> Rank : 1947 out of 4110 [Private Score : 3.62025] (47.37%)</br>\n> Rank : 2296 out of 4110 [Public Score : 3.70437] (55.86%)</br>\n\n> LGBM with 87 using median for nan (new merchant purchase date min max mode with merchant data) features.</br>\n> [1102] training's rmse: 3.47996\tvalid_1's rmse: 3.60637\n> CV RMSE score: 3.65275</br>\n> Rank : 1498 out of 4110 [Private Score : 3.61882] (36.45%)</br>\n> Rank : 2227 out of 4110 [Public Score : 3.70239] (54.18%)</br>\n\n> LGBM_KFold7 with 95_OHE using median for nan (new merchant purchase date min max mode with merchant data) features.</br>\n> Rank : 1410 out of 4110 [Private Score : 3.61813] (34.31%)</br>\n> Rank : 2185 out of 4110 [Public Score :  3.70141] (53.16%)</br>\n> CV RMSE score: 3.65106\n\n> LGBM_KFold17 with 128 features using median for nan (new merchant purchase date min max mode with merchant data).</br>\n> Rank : 1805 out of 4110 [Private Score : 3.61959] (43.92%)</br>\n> Rank : 2184 out of 4110 [Public Score :  3.70135] (53.14%)</br>\n> CV RMSE score: 3.64977\n\n> LGBM_KFold17 with 136 features using median for nan (new merchant purchase date min max mode with merchant data).</br>\n> Rank : 1655 out of 4110 [Private Score : 3.61937] (40.27%)</br>\n> Rank : 2160 out of 4110 [Public Score :  3.70097] (52.55%)</br>\n> CV RMSE score: 3.64882\n> Wall time: 33min 52s\n\n> LGBM_KFold17_merchantid with 137 features using median for nan (new merchant purchase date min max mode with merchant data, purchase date max ratio) at CV RMSE of 3.64613.</br>\n> Rank : 963 out of 4110 [Private Score : 3.61572] (23.43%)</br>\n> Rank : 1722 out of 4110 [Public Score :  3.69594] (41.90%)</br>\n> CV RMSE score: 3.64613\n\n> LGBM_KFold17_merchantid with 137 features using median for nan (new merchant purchase date min max mode with merchant data, purchase date max ratio, pd_max) at CV RMSE of 3.64625.</br>\n> Rank : 922 out of 4110 [Private Score : 3.61546] (22.43%)</br>\n> Rank : 1672 out of 4110 [Public Score :  3.69539] (40.68%)</br>\n> CV RMSE score: 3.64625\n> Early stopping, best iteration is:\n> [1152]\ttraining's rmse: 3.47992\tvalid_1's rmse: 3.43485\n> Wall time: 49min 54s\n\n> LGBM_KFold17_merchantid with 137 features using median for nan (new merchant purchase date min max mode with merchant data, 'new_hist_purchase_date_max_ratio', pd_max, 'new_hist_purchase_amount_max_ratio') at CV RMSE of 3.64680.</br>\n> Rank : 352 out of 4110 [Private Score : 3.61319] (8.56%)</br>\n> Rank : 1560 out of 4110 [Public Score :  3.69400] (37.96%)</br>\n> CV RMSE score: 3.64680\n> Early stopping, best iteration is:\n> [1263]\ttraining's rmse: 3.46726\tvalid_1's rmse: 3.43649\n> Wall time: 37min 10s\n\n[1400]\ttraining's rmse: 3.45464\tvalid_1's rmse: 3.43656\n\n## Not improving\n> Remove the 'new_merchant_purchase_date_max' and 'hist_purchase_date_max' after creating the ratio\n> Early stopping, best iteration is:\n> [1007]\ttraining's rmse: 3.49581\tvalid_1's rmse: 3.43503\n> CV RMSE score: 3.64655 \n> Wall time: 52min 27s","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\n# import xgboost as xgb\n# import lightgbm as lgb\n# from sklearn.tree import DecisionTreeRegressor\n\n# # Train classifiers\n# xgb_model = xgb.XGBRegressor(\n#     max_depth = 15,\n#     learning_rate = 0.01,\n#     n_estimators = 1000,\n#     subsample = .9,\n#     colsample_bylevel = .9,\n#     colsample_bytree = .9,\n#     min_child_weight= .9,\n#     gamma = 0,\n#     random_state = 100,\n#     booster = 'gbtree',\n#     objective = 'reg:squarederror' # 'reg:linear' deprecated\n# )\n\n# param = {'num_leaves': 111,\n#          'min_data_in_leaf': 149, \n#          'objective':'regression',\n#          'max_depth': 9,\n#          'learning_rate': 0.005,\n#          \"boosting\": \"gbdt\",\n#          \"feature_fraction\": 0.7522,\n#          \"bagging_freq\": 1,\n#          \"bagging_fraction\": 0.7083 ,\n#          \"bagging_seed\": 11,\n#          \"metric\": 'rmse',\n#          \"lambda_l1\": 0.2634,\n#          \"random_state\": 133,\n#          \"verbosity\": -1}\n\nreg1 = xgb_model\nreg2 = lgbm_reg\nreg3 = tree_reg\n\nereg = VotingRegressor([('gb', reg1), ('lr', reg3)]) # ('rf', reg2)\nereg\n\n# from sklearn.linear_model import Lasso\n# lasso_reg = Lasso(alpha=0.1)\n# lasso_reg.fit(X[train_features], y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nereg.fit(X[train_features], y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nensemble_predictions  = ereg.predict(X_test[train_features])\nensemble_predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_file_for_submission(\"xgbr_treereg_95_feats_median_KFold7.csv\", card_ids, ensemble_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -tl","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:36:03.304741Z","iopub.execute_input":"2021-09-29T12:36:03.305244Z","iopub.status.idle":"2021-09-29T12:36:04.236769Z","shell.execute_reply.started":"2021-09-29T12:36:03.305196Z","shell.execute_reply":"2021-09-29T12:36:04.235000Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"### Feature Extractions (with Sequential Feed-forward Neural Network)\n* Train our model</br>\n> For training a model, we use the fit function, which trains the model for a given number of epochs, which refers to the number of times we pass our dataset through our model to train it.  We use callbacks.TensorBoard to writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model.  To save our model after every epoch we using callbacks.ModelCheckpoint for it.","metadata":{}},{"cell_type":"code","source":"# excluded_features = ['first_active_month', 'card_id', 'target', 'hist_purchase_date_min', 'hist_purchase_date_max',\n#                      'new_merchant_purchase_date_min', 'new_merchant_purchase_date_max',\n#                      'feature_1', 'feature_2', 'feature_3', 'hist_dayofweek_nunique', 'hist_hour_nunique',\n#                      'new_merchant_weekend_sum', 'new_merchant_weekend_mean', 'hist_month_nunique',\n#                      'new_merchant_category_3_mean_mean', 'hist_merchant_id_nunique',\n#                      'hist_purchase_amount_min', 'hist_weekofyear_nunique', 'hist_installments_max',\n#                      'hist_purchase_amount_max', 'hist_category_2_mean_mean'\n#                     ]\n# train_features = [c for c in df_train.columns if c not in excluded_features]\n\n# print(\"Features used for training : \", train_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 43 out of 49 features\nX[train_features].shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tensorboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True)\nmodel_checkpoints = callbacks.ModelCheckpoint(\"weights_{epoch:02d}_{val_loss:.2f}.h5\", monitor='val_loss',\n                                              verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(input_df):\n    print(\"No. of features :\", input_df.shape[1])\n    model = models.Sequential()\n    model.add(layers.Dense(1024, activation='relu', input_shape=(input_df.shape[1],)))\n    model.add(layers.Dropout(0.2)),\n    model.add(layers.Dense(512, activation='relu'))\n    model.add(layers.Dropout(0.3)),\n    model.add(layers.Dense(32, activation='relu'))\n    model.add(layers.Dropout(0.5)),\n    model.add(layers.Dense(1))\n\n    lr_schedule = ExponentialDecay(initial_learning_rate=1e-3,\n                                   decay_steps=10000, decay_rate=0.9)\n    \n    model.compile(optimizer=Adam(learning_rate=1e-3), # Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n                  loss='mean_squared_error', # mse\n                  metrics=['RootMeanSquaredError'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import keras\n# from keras import callbacks\n# from keras.models import Sequential\n# from keras.layers import Dense, Dropout, BatchNormalization\n# from tensorflow.keras import models, layers\n# # from keras.optimizers import RMSprop, SGD\n# from keras.optimizers import Adam, Adadelta\n# from keras.optimizers.schedules import ExponentialDecay\n# from keras.metrics import RootMeanSquaredError\n\ndef build_model(input_df):\n    print(\"No. of features :\", input_df.shape[1])\n    model = models.Sequential()\n\n    model.add(Dense(2 ** 10, input_dim = input_df.shape[1],\n                    kernel_initializer='glorot_uniform', activation='relu')) # init='random_uniform',\n    model.add(Dropout(0.25))    \n    model.add(BatchNormalization())\n    model.add(Dense(2 ** 9, activation='relu')) # kernel_initializer='random_uniform'\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25)) \n    model.add(Dense(2 ** 5, activation='relu')) # kernel_initializer='random_uniform'\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))      \n    model.add(Dense(1))\n\n    #     model.compile(loss='mean_squared_error', optimizer='adam')\n    model.compile(optimizer=Adam(learning_rate=1e-3), # Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n                  loss='mse',\n                  metrics=['RootMeanSquaredError'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl_model = build_model(X[train_features])\n\ndl_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_log = dl_model.fit(X[train_features], y, epochs=10, batch_size=64, # 16\n                         validation_split = 0.2,\n                         callbacks=[model_checkpoints]) # , verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n\nax1.plot(model_log.history['root_mean_squared_error'])\nax1.plot(model_log.history['val_root_mean_squared_error'])\nax1.set_title('Root Mean Squared Error (RMSE)')\nax1.set(xlabel='Epoch', ylabel='RMSE')\nax1.legend(['train', 'validation'], loc='upper right')\nax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n\nax2.plot(model_log.history['loss'])\nax2.plot(model_log.history['val_loss'])\nax2.set_title('Loss (Lower Better)')\nax2.set(xlabel='Epoch', ylabel='Loss')\nax2.legend(['train', 'validation'], loc='upper right')\nax2.xaxis.set_major_locator(MaxNLocator(integer=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_batchdata = X[train_features].iloc[:5]\nfirst_batchlabels = y.iloc[:5]\n\nprint(\"XBGR Predictions: \", xgb_model.predict(first_batchdata))\nprint(\"LGBM Predictions: \", lbg_reg.predict(first_batchdata))\nprint(\"DL Predictions: \", dl_model.predict(first_batchdata))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Labels: \", list(first_batchlabels))\n\ntest_mse_score, test_rmse_score = dl_model.evaluate(first_batchdata, first_batchlabels)\n\nprint(f'test_mse_score : {test_mse_score}')\nprint(f'test_rmse_score : {test_rmse_score}')\n\n# The attribute model.metrics_names will give you the display labels for the evalution outputs.\nprint(\"\\nModel metrics :\", dl_model.metrics_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[train_features].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl_predictions = dl_model.predict(df_test[train_features])\ndl_predictions.shape, df_test.shape, df_test[train_features].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_file_for_submission(\"DL5_141feat_adam_rmse_128NN.csv\", card_ids, dl_predictions[:, 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr_predictions_df = pd.DataFrame(xgbr_predictions, columns = ['XGBR_Pred'])\nxgbr_predictions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_predictions_df = pd.DataFrame(lgbm_predictions, columns = ['LGBM_Pred'])\nlgbm_predictions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtreg_predictions_df = pd.DataFrame(tree_reg_predictions, columns = ['DTR_Pred'])\ndtreg_predictions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df = pd.concat([xgbr_predictions_df, lgbm_predictions_df, dtreg_predictions_df], axis=1, sort=False)\nconcat_pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfb_predictions_df = pd.DataFrame(rf_predictions, columns = ['RFB_Pred'])\nrfb_predictions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df = pd.concat([concat_pred_df, rfb_predictions_df], axis=1, sort=False)\nconcat_pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df.drop(columns=['Avg_Pred', 'DTR_Pred'], axis=1, inplace=True)\nconcat_pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl_predictions_df = pd.DataFrame(dl_predictions[:, 0], columns = ['DL_Pred'])\ndl_predictions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(concat_pred_df), type(dl_predictions_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df = pd.concat([concat_pred_df, dl_predictions_df], axis=1, sort=False)\nconcat_pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df['Avg_Pred'] = concat_pred_df.mean(axis=1)\nconcat_pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df['Avg_Pred'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[train_features].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average predicted values from XGBR, LGB and DL\ncreate_file_for_submission(\"xgbr_lgbm_rfb_85feat.csv\", card_ids, concat_pred_df['Avg_Pred'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ensemble Learning with weights (Attempt novelty)","metadata":{}},{"cell_type":"code","source":"# Load from saved files\n# xgbr_preds = pd.read_csv('../input/testpredictions/xgbr_79_feats.csv')\n# lgbm_preds = pd.read_csv('../input/testpredictions/lgb_79_feats.csv')\n# dl_preds = pd.read_csv('../input/testpredictions/xgbr_lgbm_dl_79feat.csv')\n\n# concat_pred_df = pd.concat([xgbr_preds['target'], lgbm_preds['target'], dl_preds['target']],\n#                             axis=1, keys=['XGBR_Pred', 'LGBM_Pred', 'DL_Pred'])\n\n# concat_pred_df['Avg_Pred'] = concat_pred_df.mean(axis=1)\n\n# xgbr_preds.loc[:5, 'target'], lgbm_preds.loc[:5, 'target'], dl_preds.loc[:5, 'target']\n# concat_pred_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr_predictions.shape, lgbm_predictions.shape, tree_reg_predictions.shape, rf_predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_pred_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_reg_predictions_df = pd.DataFrame(ensemble_reg_pred, columns = ['ESMR_Pred'])\nensemble_reg_predictions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_samples = 20\nxgbr_pred_samples = concat_pred_df.loc[:no_of_samples, 'XGBR_Pred']\nlgbm_pred_samples = concat_pred_df.loc[:no_of_samples, 'LGBM_Pred']\nrfb_pred_samples = concat_pred_df.loc[:no_of_samples, 'RFB_Pred']\n# dtreg_pred_samples = concat_pred_df.loc[:no_of_samples, 'DTR_Pred']\n\nmodel_avg_pred_samples = concat_pred_df.loc[:no_of_samples, 'Avg_Pred']\nesm_reg_pred_samples = ensemble_reg_predictions_df.loc[:no_of_samples, 'ESMR_Pred']\n\n# https://matplotlib.org/stable/tutorials/colors/colors.html\n# https://matplotlib.org/stable/api/markers_api.html\n\nplt.figure(figsize=(20,12))\nplt.plot(xgbr_pred_samples, 'gd', label='XGBR')\nplt.plot(lgbm_pred_samples, 'b^', label='LGBM')\nplt.plot(dtreg_pred_samples, 'ys', label='RFB') # DL5\nplt.plot(model_avg_pred_samples, 'r*', ms=10, label='Average')\n\nplt.plot(esm_reg_pred_samples, 'm.', ms=10, label='Average')\n\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\n\nplt.ylabel('predicted')\nplt.xlabel('Test samples')\nplt.legend(loc=\"best\")\nplt.title('Model predictions and their average')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_samples = 20\nxgbr_pred_samples = xgbr_preds.loc[:no_of_samples, 'target']\nlgbm_pred_samples = lgbm_preds.loc[:no_of_samples, 'target']\ndl_pred_samples = dl_preds.loc[:no_of_samples, 'target']\nmodel_avg_pred_samples = concat_pred_df.loc[:no_of_samples, 'Avg_Pred']\n\nplt.figure(figsize=(20,12))\nplt.plot(xgbr_pred_samples, 'gd', label='XGBR')\nplt.plot(lgbm_pred_samples, 'b^', label='LGBM')\nplt.plot(dl_pred_samples, 'ys', label='DL5')\nplt.plot(model_avg_pred_samples, 'r*', ms=10, label='Average')\n\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\n\nplt.ylabel('predicted')\nplt.xlabel('Test samples')\nplt.legend(loc=\"best\")\nplt.title('Model predictions and their average')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgbr_train_pred = xgb_model.predict(X[train_features])\nlgbm_train_pred = lgbm_reg.predict(X[train_features])\nrfb_train_pred = rf_best.predict(X[train_features])\n\n# dtreg_train_pred = tree_reg.predict(X[train_features])\n\n# dl_train_pred = dl_model.predict(X[train_features])\n\nxgbr_train_pred_df = pd.DataFrame(xgbr_train_pred, columns = ['XGBR_Tr_Pred'])\nlgbm_train_pred_df = pd.DataFrame(lgbm_train_pred, columns = ['LGBM_Tr_Pred'])\nrfb_train_pred_df = pd.DataFrame(rfb_train_pred, columns = ['RFB_Tr_Pred'])\n\n# dtreg_train_pred_df = pd.DataFrame(dtreg_train_pred, columns = ['DTR_Tr_Pred'])\n\n# xgbr_train_pred_df, lgbm_train_pred_df, dtreg_train_pred_df and dl_train_pred_df\nconcat_train_pred = pd.concat([xgbr_train_pred_df, lgbm_train_pred_df, rfb_train_pred_df], axis=1, sort=False)\nconcat_train_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Linear Regression model to have ensemble regression\nfrom sklearn.tree import DecisionTreeRegressor\n# Cost : Root Mean Square Error, RMSE\nfrom sklearn.metrics import mean_squared_error\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(concat_train_pred, y)\n\ntrain_pred = tree_reg.predict(concat_train_pred)\ntree_mse = mean_squared_error(y, train_pred)\ntree_rmse = np.sqrt(tree_mse)\n\nprint(\"DecisionTreeRegressor RMSE :\", tree_rmse)\n\n# scores = cross_val_score(tree_reg, concat_train_pred, y,\n#                          scoring=\"neg_mean_squared_error\", cv=10)\n\n# tree_rmse_scores = np.sqrt(-scores) # opposite of MSE, need to have negative sign\n\n# # Different results when it is executed\n# display_scores(tree_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = concat_pred_df.drop('Avg_Pred', axis=1)\ntest_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_reg_pred = tree_reg.predict(test_pred)\nensemble_reg_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[train_features].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using a trained linear regressor with predicted values from XGBR, LGB and DL\ncreate_file_for_submission(\"xgbr_lgbm_rfb_85feat_reg.csv\", card_ids, ensemble_reg_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_targets = train_data[\"target\"].copy()\n# df_train = train_data.drop(columns=['card_id', 'first_active_month', 'target'])\n\n# df_train.shape, train_targets.shape\n\n# --\n# X_test = test_data.drop(columns=['card_id', 'first_active_month'])\n\n\n# --\n# card_ids = test_data[\"card_id\"].copy()\n\n# card_ids.shape\n# --\n# kaggle = pd.DataFrame({'card_id': card_ids, 'target': xtest_predictions[:, 0]})\n# kaggle.to_csv('./elo_dl_pred.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download csv file\n# <a href=\"./elo_pred.csv\"> Download File </a>","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Exploring the train and test data files :</h2>","metadata":{}},{"cell_type":"code","source":"# In order to read .xlsx file\n# !pip install openpyxl\n# Read excel-formatted data dictionary file with pandas\n# data_dictionary=pd.read_excel('../input/elo-merchant-category-recommendation/Data Dictionary.xlsx')\n# data_dictionary","metadata":{"id":"JUa3xmlORIHo","outputId":"06467807-1a7e-41bb-de36-f72461da80ba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* This DataDictionary file have the description of all the features in Description column which were included in train.csv.\n\n* From second row we have columns which have the description of all the columns in our data and third row tell us about the card_id and third one is about the first_active_month which tell us about the month and year of purchase of products.\n\n* feature_1, feature_2, feature_3 has categorical value which is in row fourth,fifth,and sixth.\n\n* last row tells us about the prediction on the basis of these features which is known as target column. or we can say loyalty score which is calculated after the two months.","metadata":{"id":"CG17BrtARIHp"}},{"cell_type":"code","source":"print('The number of rows in train_data is:',train_data.shape[0])\nprint('The number of rows in test_data is:',test_data.shape[0])\nplt.bar([0,1],[train_data.shape[0],test_data.shape[0]])\nplt.xticks([0,1],['train_rows','test_rows'])","metadata":{"id":"a53tV2-yRIHq","outputId":"ed3f0fac-2908-4be0-f182-8acd9dfa126c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"id":"PujuFJNFRIHr","outputId":"1d60fc85-47fd-4cb8-a7f3-c6012cc1f3de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"id":"pt97htPpRIHr","outputId":"c3bc9753-836f-4968-ba54-a2a230cec7fe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()\nprint(\"********************************************************************\")\ntest_data.info()","metadata":{"id":"29nzghCmRIHs","outputId":"f13484a5-ded8-4c1e-d786-8b6395c9a47c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obsaervations :**\n\n* The main data train has 6 values. 'first_active_month', 'card_id', 'feature_1', 'feature_2', 'feature_3', 'target'.\n* first_active_month : This is active_month for card_id. \n* feature_1,2,3 : it is key important but hidden meaning.\n* target : Loyalty numerical score calculated 2 months after historical and evaluation period\n* We can infer that both the data have same columns and overall same structure. So, We will explore both data simultaneously.\n","metadata":{"id":"IVyRhgtjRIHt"}},{"cell_type":"markdown","source":"**Missing values in train and test data :\n(Check for nan values in the whole train and test data)**","metadata":{"id":"gdREgDzyRIHt"}},{"cell_type":"code","source":"train_data.isna().any()","metadata":{"id":"OkpdGONWRIHt","outputId":"3a7b2db2-cb0e-4835-95a3-ddc6d6bdf48f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** In train Data there is no nan values for any features in train data","metadata":{"id":"K0r45B4lRIHu"}},{"cell_type":"code","source":"test_data.isna().any()","metadata":{"id":"ctdhOSY6RIIC","outputId":"a7184b94-2f31-4988-e800-aa3b922032bc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[test_data['first_active_month'].isna()]","metadata":{"id":"WP7zctWsRIID","outputId":"f5492a3d-b3d1-46d4-e724-62db644df5ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :**\nIn the Test Data, there is one row with 'first_active_month' as nan value. Since it is test data we have to impute the value.","metadata":{"id":"4rbmAw0iRIIE"}},{"cell_type":"markdown","source":"**Feature comparison in train and test data features :**","metadata":{"id":"2omwD0ebRIIE"}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize = (15, 5));\ntrain_data['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='feature_1', rot=0);\ntrain_data['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='feature_2', rot=0);\ntrain_data['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='feature_3', rot=0);\nplt.suptitle('Counts of categories for train features');\n\nfig, ax = plt.subplots(1, 3, figsize = (15, 5));\ntest_data['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='feature_1', rot=0);\ntest_data['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='feature_2', rot=0);\ntest_data['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='feature_3', rot=0);\nplt.suptitle('Counts of categories for test features');","metadata":{"id":"05KYCGYjRIIO","outputId":"43a42dd3-e97e-4a34-d067-16f0a3a2e7d3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* We can see from above plots that test and train data are distributed similarly.\n* feature_1, feature_2, feature_3, all are categorical variables\n* feature_1 has 5 unique values\n* feature_2 has 3 unique values\n* feature_3 is a binary column","metadata":{"id":"-OzETm0IRIIP"}},{"cell_type":"code","source":"# No. of unique values; not all target values are unique (total number : 201917)\ntrain_data['feature_1'].nunique(), train_data['feature_2'].nunique(), train_data['feature_3'].nunique(), train_data['target'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Anonymised Features Analysis : feature_1, feature_2, feature_3**\n\n**checking distributions with target :**","metadata":{"id":"7ttj5M4mRIIP"}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(131)\nsns.kdeplot(x ='target',data = train_data,hue = 'feature_1',palette='rainbow')\nplt.title('Pdf of target over different categories of Feature_1')\nplt.subplot(132)\nsns.kdeplot(x ='target',data = train_data,hue = 'feature_2',palette='Dark2_r')\nplt.title('distribution of target over different categories of Feature_2')\nplt.subplot(133)\nsns.kdeplot(x ='target',data = train_data,hue = 'feature_3',palette='Dark2_r')\nplt.title('distribution of target over different categories of Feature_3')\nplt.show()","metadata":{"id":"Pv3SDeF6RIIQ","outputId":"780d9c10-88fe-434a-8347-f8a67d6d86d1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** \n\nThe above two plots show a key point : \n\n* while different categories of these features could have various counts, the distribution of target is almost the same. This could mean, that these features aren't really good at predicting target - we'll need other features and feature engineering. Also it is worth noticing that mean target values of each catogory of these features is near zero. This could mean that data was sampled from normal distribution.\n\n**Note:** The same information can be gathered by using box-plot and violin-plot, I have tried all of them. Here, I use kdeplot as I found it more visually appealing. In further analysis I have used Box-plot more often.","metadata":{"id":"r6kJblypRIIQ"}},{"cell_type":"markdown","source":"**let's see Target column seperately :**","metadata":{"id":"xEXLjUXBz3cQ"}},{"cell_type":"code","source":"train_data['target'].describe()","metadata":{"id":"ifk4NZF-RIIG","outputId":"f700673b-25fa-4ae9-f30d-b4cd1db29014","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the estimated pdf of target variable with kernel density estimation (KDE)\nsns.kdeplot(train_data['target'])\nplt.title(\"PDF of Target\")\nplt.show()","metadata":{"id":"rKlr4FQwWDyM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** The target value is almost normally distributed with bunch of outlier value near -30. This distribution indicates that the target value is normalized with pre-decided mean and standard deviation.\n\nThis outlier value of the target is a value which needs more look into the feature EDA to understand cause of it.","metadata":{"id":"3vc9yk_0WKpH"}},{"cell_type":"markdown","source":"**Analyze the outliers :**\n\n","metadata":{"id":"cV3-vUFUWR3n"}},{"cell_type":"code","source":"loyality_score = train_data['target']\nax = loyality_score.plot.hist(bins=20, figsize=(6, 5))\n_ = ax.set_title(\"target histogram\")\nplt.show()\n\nfig, axs = plt.subplots(1,2, figsize=(12, 5))\n_ = loyality_score[loyality_score > 10].plot.hist(ax=axs[0])\n_ = axs[0].set_title(\"target histogram for values greater than 10\")\n_ = loyality_score[loyality_score < -10].plot.hist(ax=axs[1])\n_ = axs[1].set_title(\"target histogram for values less than -10\")\nplt.show()\n","metadata":{"id":"_71V2JpPRIIH","outputId":"8784b21d-c5c2-4912-bcfd-5605e1063404","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* Values range from -33.2 to 17.9\n\n* -33 seems like an outlier as can be seen in the 3rd plot\n\n* other values less than -10 also seem like outliers due to very less in number\n\n* All values above 10 are also looking like outliers","metadata":{"id":"UJD2QyK8RIII"}},{"cell_type":"code","source":"target_sign = loyality_score.apply(lambda x: 0 if x <= 0 else 1)\ntarget_sign.value_counts()","metadata":{"id":"Qi5C-4CtRIIJ","outputId":"c077d3f9-9988-4898-c597-4fa6ecbce540","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** Negative and positive target values are almost in the same proportion","metadata":{"id":"VylT8G_pRIIJ"}},{"cell_type":"code","source":"outliers_in_target= train_data.loc[(train_data['target']< -10) | (train_data['target']>10)]\nprint(' The number of outliers in the data is:',outliers_in_target.shape[0])\nnon_outliers_in_target= train_data.loc[(train_data['target'] >=-10) & (train_data['target']<=10)]\nprint(' The number of non-outliers in the data is:',non_outliers_in_target.shape[0])","metadata":{"id":"GIxf_uSvRIIK","outputId":"e691244c-d2ae-4fe6-e70f-ca1ea73551c2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outliers comparison with the feature of target :","metadata":{"id":"kghWmbSTWgv9"}},{"cell_type":"code","source":"plt.figure(figsize=[16,9])\nplt.suptitle('Outlier vs. non-outlier feature distributions', fontsize=20, y=1.1)\n\nfor num, col in enumerate(['feature_1', 'feature_2', 'feature_3', 'target']):\n    if col is not 'target':\n        plt.subplot(2, 3, num+1)\n        non_outlier = non_outliers_in_target[col].value_counts() / non_outliers_in_target.shape[0]\n        plt.bar(non_outlier.index, non_outlier, label=('non-outliers'), align='edge', width=-0.3, edgecolor=[0.2]*3,color=['teal'])\n        outlier = outliers_in_target[col].value_counts() / outliers_in_target.shape[0]\n        plt.bar(outlier.index, outlier, label=('outliers'), align='edge', width=0.3, edgecolor=[0.2]*3,color=['brown'])\n        plt.title(col)\n        plt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"tZp-PSWbWRZJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* We can see There are only slight differences between outliers and non-outliers, but they don't seem to be that big and they certainly can't explain the difference between the target values, at least based on the features in the train dataset. It means the card_id's having outliers as loyality score having pretty much similar properties to the regular ones.\n\n* Outliers could be one of the main purposes of this competition. May be those represent fraud or credit default etc. i.e. they are important. The target variable is normally distributed, and outliers seem to be purposely introduced in the loyalty formula. \n\n* As noted in multiple threads over kaggle, more than half of the RMSE is due to the outliers with loyalty scores of ~ -33. They strongly mention, If we try to replace these outliers with the median, retrain the model and submit, we will\nfind our leaderboard score WORSE than if we keep the outliers at their original values. Impute any values will significantly affect the RMSE score for test set. So, imputations have been excluded. This tells us that outliers are included in the test set. Furthermore, given the magnitude of the impact of outliers on the RMSE score, Our focus should be on predicting those outliers as accurately as possible.\n\n* For mitigating the impact of outliers, We can make the outliers as a binary feature whether card's target value is outliers or not. So that while training our model can learn that given entry has target score as outlier or not and use this information while predicting loyality score.","metadata":{"id":"MoZusVaiXPkA"}},{"cell_type":"markdown","source":"**Analysis of feature First_active_month :**","metadata":{"id":"NM7vpCSiRIIQ"}},{"cell_type":"markdown","source":"Distribution of first_active_month across years :","metadata":{"id":"GDH44ijXsL0O"}},{"cell_type":"code","source":"year_train = train_data['first_active_month'].value_counts().sort_index()\nyear_test = test_data['first_active_month'].value_counts().sort_index()\nax = year_train.plot(figsize=(10, 5))\nax = year_test.plot(figsize=(10, 5))\n_ = ax.set_xticklabels(range(2010, 2020))\n_ = ax.set_title(\"Distribution across years\")\n_ = ax.legend(['train', 'test'])","metadata":{"id":"khsd-9KkRIIR","outputId":"548af790-2598-495d-e6c5-a8d41d30e109","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** Years range from 2011 to 2018. But, Most of the data lies in the years ranging from 2016 to 2018 and trends of counts for train and test data are similar.","metadata":{"id":"M5YF7ydQRIIR"}},{"cell_type":"markdown","source":"Distribution of first_active_month across months :","metadata":{"id":"Wql2UNyDsa5E"}},{"cell_type":"code","source":"train_data['first_active_month'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['first_active_month'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Extraction\ntrain_data[\"month\"] = train_data['first_active_month'].str.split(\"-\").str[1]\ntrain_data.head()","metadata":{"id":"m4WOOqSh8lu6","outputId":"4563ecd9-396c-4639-cd19-de16496722e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = train_data['month'].value_counts().sort_index()\nax = temp.plot()\n_ = ax.set_xticklabels(range(-1, 15, 2))\n_ = ax.set_title(\"Distribution across months\")","metadata":{"id":"hluPldyGryey","outputId":"2e729c35-fb1d-4d30-fe04-2a9757e486ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** Last 6 months (July to December) has relatively more data than first 6 months (January to June).","metadata":{"id":"naVOZ2SJRIIV"}},{"cell_type":"markdown","source":"**First_active_month Vs Target variable :**","metadata":{"id":"ENA-mIVBRIIX"}},{"cell_type":"code","source":"train_data['first_active_month'] = pd.to_datetime(train_data['first_active_month'],\n                                                  format='%Y-%m')","metadata":{"id":"kGhZVUAZRIIW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['first_active_month'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 8))\n\nsns.boxplot(x=train_data[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation between variables : Pearson Correlation (Bi-variate)**","metadata":{}},{"cell_type":"code","source":"features_correlation = pd.DataFrame(train_data, columns=['feature_1', 'feature_2', 'feature_3', 'target'])\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(10,8))\nplt.title('Pearson Correlation of anonymise Features with Target', y=1.05, size=15)\nsns.heatmap(features_correlation.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(x = train_data['first_active_month'], y= train_data['target'])\nplt.title(\"Distribution of target over first_active_month\")\nplt.show()","metadata":{"id":"7tUdyZehRIIX","outputId":"dfa4c63b-e378-4cca-f498-924309309f72","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* The above plot reveals that the target variable (loyalty score) behaves like a damping frequency plot. And it is mentioned in the Buisness problem that the target score is calcuated with the recent year transactions.\n\n* Older Card's: The cards which have first active month from 2012 to 2015.\n\n* new card's: The cards which have first active month from 2015 to 2018.\n\n* The Older card's have large number of transactions which affects the target towards the negative value. and the new card's have transactions which affects the target towards positive value.\n\nSo, I think the type of transactions by the newer card's is different from the older card's which helps in increase the loyalty Score.\n\n","metadata":{"id":"_qixIUMHRIIX"}},{"cell_type":"markdown","source":"**Correlation between variables : Variance Inflation Factor (Multicollinearity)**","metadata":{"id":"ng78t9auKeKe"}},{"cell_type":"markdown","source":"Reference : https://www.statisticshowto.com/variance-inflation-factor\n\n**Abstract :**\n\nMulticollinearity occurs when two or more independent variables are highly correlated with one another in a regression model.  Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.\n\nA variance inflation factor(VIF) detects multicollinearity in regression analysis. Multicollinearity is when there’s correlation between predictors (i.e. independent variables) in a model; it’s presence can adversely affect your regression results. The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model.\n\n- VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.\n- VIF score of an independent variable represents how well the variable is explained by other independent variables.\n- VIF = 1 / (1 - R^2)\n- R^2 means that the variable is highly correlated with the other variables\n- The closer R^2 value is to 1, the higher the value of VIF and the higher the multicollinearity with the particular independent variable\n- Although correlation matrix and scatter plots can also be used to find multicollinearity, their findings only show the bivariate relationship between the independent variables.  VIF is preferred as it can show the correlation of a variable with a group of other variables.\n- Dropping variables should be an iterative process starting with the variable having the largest VIF value because its trend is highly captured by other variables. If you do this, you will notice that VIF values for other variables would have reduced too, although to a varying extent\n- When you care more about how much each individual feature rather than a group of features affects the target variable, then removing multicollinearity may be a good option\n- If multicollinearity is not present in the features you are interested in, then multicollinearity may not be a problem\n- Knowledge about multicollinearity can be quite helpful when you’re building interpretable machine learning models\n\nA rule of thumb for interpreting the variance inflation factor:\n\n1 = not correlated.\nBetween 1 and 5 = moderately correlated.\nGreater than 5 = highly correlated.\n\nExactly how large a VIF has to be before it causes issues is a subject of debate. What is known is that the more your VIF increases, the less reliable your regression results are going to be. In general, a VIF above 10 indicates high correlation and is cause for concern. Some authors suggest a more conservative level of 2.5 or above.","metadata":{}},{"cell_type":"code","source":"#Finding Correlation between variables of train_data features\nselected_columns = ['feature_1','feature_2','feature_3']\ndata_frame = train_data[selected_columns]\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(data_frame.iloc[:,:].values, i) for i in range(data_frame.shape[1])]\nvif[\"features\"] = data_frame.columns\nvif","metadata":{"id":"mTh-54awORHz","outputId":"d07e67c6-464e-4ae8-a3e4-f80c56e1e2f4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding Correlation between variables of test_data features\nselected_columns = ['feature_1','feature_2','feature_3']\ndata_frame = test_data[selected_columns]\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(data_frame.iloc[:,:].values, i) for i in range(data_frame.shape[1])]\nvif[\"features\"] = data_frame.columns\nvif","metadata":{"id":"P14xqDh9CW1Z","outputId":"9a688da3-0aaa-4592-db6b-a19bae8b4fe7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** \n\n* The VIF values for all the three features are well under 10. So, there is no problem of multicollinearity in the train data and test data.\n* Also VIF values are very near to 0, which interpret that features are not at all correlated.","metadata":{"id":"bsnwMxIjRIId"}},{"cell_type":"markdown","source":"<h2>Exploring the historical_transactions and new_merchant_transactions data files :</h2>","metadata":{"id":"VXItVYRKRIIe"}},{"cell_type":"code","source":"# Read excel-formatted data dictionary file with pandas\n#data_dictionary = pd.read_excel('../input/elo-merchant-category-recommendation/Data_Dictionary.xlsx', sheet_name='history')\n#data_dictionary","metadata":{"id":"VD4gxy7URIIe","outputId":"b3b81d5b-a7aa-4d03-a3d1-de0c553c2f25","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read excel-formatted data dictionary file with pandas\n#data_dictionary = pd.read_excel('../input/elo-merchant-category-recommendation/Data_Dictionary.xlsx', sheet_name='new_merchant_period')\n#data_dictionary","metadata":{"id":"Wywh9fQcRIIf","outputId":"0c381563-e2ae-4746-f14e-39c155310c5c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** After going through \"history\" and \"new_merchant_period\" worksheets in Data Dictionary.xlsx, We can infer that both the data have same columns and overall same structure. So, We will Explore both data side by side.","metadata":{"id":"ILRaFfSVRIIf"}},{"cell_type":"code","source":"print(f'{historical_data.shape[0]} rows in historical transactions!\\n')\nhistorical_data.head()","metadata":{"id":"c8eLDwlgRIIg","outputId":"7dc9f1fa-14b9-42d5-cf14-6c466d166e14","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{newmerchant_data.shape[0]} rows in new merchants data!\\n')\nnewmerchant_data.head()","metadata":{"id":"1ry56I4_RIIj","outputId":"475b11ad-190b-4f79-862d-a3a42e0188c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\nWe can see that there are:\n\n* 6 features type ID: card_id, merchant_category_id, subsector_id, merchant_id, city_id, state_id\n\n* 2 features type integer/counter: month_lag, installments\n\n* 1 feature type numerical: purchase_amount\n\n* 1 feature type date: purchase_date\n\n* 4 features type categorical: authorized_flag, category_3, category_1, category_2","metadata":{"id":"RG0eFlWRRIIh"}},{"cell_type":"code","source":"# By default, Non-Null Counts are shown only if the DataFrame is smaller than\n# pandas.options.display.max_info_rows and pandas.options.display.max_info_columns.\nhistorical_data.info(show_counts=True)","metadata":{"id":"ipSihUT9RIIg","outputId":"1b5a5235-ff8d-4623-f0f7-b8cb98ed5a5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# category_3, merchant_id and category_2 have null values\nhistorical_data.isna().any()","metadata":{"id":"ksJKQh16RIIj","outputId":"845d10ae-069c-4cee-e168-f4e8fab44a95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data.info(show_counts=True)","metadata":{"id":"S49nv8PbRIIg","outputId":"6f62cba1-a389-4b33-c6ae-f1680d53d283","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# category_3, merchant_id and category_2 have null values\nnewmerchant_data.isna().any()","metadata":{"id":"ijrfqyR9RIIk","outputId":"bcf4af76-45cb-4d9a-fefc-78e9999d3e6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** Both historical_transaction and new_merchant_transaction have Nan values in same columns which are : merchand_id, category_2, category_3.","metadata":{"id":"R_xNQXCVRIIk"}},{"cell_type":"markdown","source":"**Analysis of Category Features : category_1,category_2 and category_3**","metadata":{"id":"263JX8joRIIk"}},{"cell_type":"code","source":"print('Value counts for category features of Historical Transactions :\\n')\nprint(historical_data['category_1'].value_counts())\nprint('*****************************')\nprint(historical_data['category_2'].value_counts())\nprint('*****************************')\nprint(historical_data['category_3'].value_counts())\n\nprint('\\nValue counts for category features of New merchant Transactions :\\n')\nprint(newmerchant_data['category_1'].value_counts())\nprint('*****************************')\nprint(newmerchant_data['category_2'].value_counts())\nprint('*****************************')\nprint(newmerchant_data['category_3'].value_counts())","metadata":{"id":"gaxepnUZ9hOW","outputId":"6a00bb48-3958-45a5-941d-2e611f94054f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize = (15, 5));\nhistorical_data['category_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='category_1', rot=0);\nhistorical_data['category_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='category_2', rot=0);\nhistorical_data['category_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='category_3', rot=0);\nplt.suptitle('Counts for category features of Historical Transactions New merchant Transactions');\n\n\nfig, ax = plt.subplots(1, 3, figsize = (15, 5));\nnewmerchant_data['category_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='category_1', rot=0);\nnewmerchant_data['category_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='category_2', rot=0);\nnewmerchant_data['category_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='category_3', rot=0);\nplt.suptitle('Counts for category features of New merchant Transactions');","metadata":{"id":"aBmrNignRIIk","outputId":"ca03803e-b98f-49e9-9237-e849689195a6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation : No data drift**\n\nThe distribution of these three category features are almost \nidentical in historical and new transactions.This shows these Category feature represent the inately charcterstics of the transactions which is constant over the period.  So, these features can be an importance feature in the decision function on final model.","metadata":{"id":"I2gRu3kGRIIl"}},{"cell_type":"markdown","source":"**Distrbution of target over categorical features :**\n\n**Note :** The train.csv file only has the target value, which is the feature we are gonna predict with models build in the future But, transactions data don't have the target values in it for each card_id's. By merging the \"target\" feature with the transactions data will help in Data analysis to fully understand different features in transactional dataFrame.","metadata":{"id":"WBcgIDHFRIIl"}},{"cell_type":"code","source":"train_data.shape, historical_data.shape, newmerchant_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stable documentation : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html#pandas.merge\n# merging target value of card_id for each transaction in historical_transactions Data\nhistorical_data = pd.merge(historical_data, train_data[['card_id','target']], how = 'outer', on = 'card_id')\n\n# merging target value of card_id for each transaction in new_merchants_transactions Data\nnewmerchant_data = pd.merge(newmerchant_data, train_data[['card_id','target']], how = 'outer', on = 'card_id')","metadata":{"id":"S4yzLugoRIIl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data.shape, newmerchant_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data.head()","metadata":{"id":"7pKzB1bi-nSD","outputId":"bdefeffe-7f6d-4807-d6b5-af6239467f33","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data.head()","metadata":{"id":"v4yEPR0V-75o","outputId":"9612069d-4a38-455d-8fe5-08b991c2bf89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,10))\nplt.subplot(231)\nsns.kdeplot(x ='target',data = historical_data,hue = 'category_1',palette='Dark2_r')\nplt.title(\"Distribution of target over Category_1 in historical data\")\nplt.subplot(232)\nsns.kdeplot(x ='target',data = historical_data,hue = 'category_2',palette='Dark2_r')\nplt.title(\"Distribution of target over Category_2 in historical data\")\nplt.subplot(233)\nsns.kdeplot(x ='target',data = historical_data,hue = 'category_3',palette='rainbow')\nplt.title(\"Distribution of target over Category_3 in historical data\")\nplt.subplot(234)\nsns.kdeplot(x ='target',data = newmerchant_data,hue = 'category_1',palette='Dark2_r')\nplt.title(\"Distribution of target over Category_1 in new_merchent data\")\nplt.subplot(235)\nsns.kdeplot(x ='target',data = newmerchant_data,hue = 'category_2',palette='Dark2_r')\nplt.title(\"Distribution of target over Category_2 in new_merchent data\")\nplt.subplot(236)\nsns.kdeplot(x ='target',data = newmerchant_data,hue = 'category_3',palette='rainbow')\nplt.title(\"Distribution of target over Category_3 in new_merchent data\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"hCociuMURIIm","outputId":"9693b647-3e7c-43ea-b509-a0ea48d3bbc8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* These three category features doesn't explicity help to differentiate the target Score(Loyalty Score). Every category have outliers in each of the sub_categories. And Almost all the category have Same IQR range.\n\n* These anonymous features doesn't reveal any important info for further feature engineering of these categories.\n\n**Note:** The same information can be gathered by using box-plot and violin-plot, I have tried all of them. Here, I use kdeplot as I found it more visually appealing. In further analysis I have used Box-plot more often.\n","metadata":{"id":"GoES7aa4RIIm"}},{"cell_type":"code","source":"# There are card_id without target\nnewmerchant_data.info(show_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Authorized Flag Feature Analysis :**","metadata":{"id":"pVOZpPiiRIIn"}},{"cell_type":"code","source":"print('Value counts for Authorized Flag of Historical Transactions :')\nprint(historical_data['authorized_flag'].value_counts())\nprint('*************************************************************')\nprint('Value counts for Authorized Flag of New Merchant Transactions :')\nprint(newmerchant_data['authorized_flag'].value_counts())\n\n#barplot for the authorized_flag feature\nfig, ax = plt.subplots(1, 2, figsize = (12, 5));\nhistorical_data['authorized_flag'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='\\nauthorized_flag(historical_transactions)');\nnewmerchant_data['authorized_flag'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='\\n   authorized_flag(new_merchant_transactions)');","metadata":{"id":"2CWQu4Hf5Eo7","outputId":"1bd45941-c948-448f-c262-59615c712772","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* The new transactions have no \"N\" category in authorized_flag. This historical transactions have both \"Y\" and \"N\".\n\n* The authorized_flag 'Y' if approved, 'N' if denied - whether the transaction is approved or Denied.\n\n* If we calculate percentage of authorized transaction in historical transaction. At average 91.3545% transactions are authorized.\n\n* This feature is an important feature for predicting the Loyalty score. because, if the card's transactions are approved most of time, there is a great chance the cards can have high Loyalty Score","metadata":{"id":"-_0lKueBRIIn"}},{"cell_type":"markdown","source":"Distributions of target over authorized flag :","metadata":{"id":"-M_z9pbNRIIo"}},{"cell_type":"code","source":"plt.figure(figsize = (14,5))\nplt.subplot(121)\nsns.boxplot(y = 'target',x= 'authorized_flag', data = historical_data)\nplt.title(\"Distributions of target over authorized flag(historical_transactions)\")\nplt.subplot(122)\nsns.boxplot(y = 'target',x= 'authorized_flag', data = newmerchant_data)\nplt.title(\"Distributions of target over authorized flag(new_merchant_transactions)\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"mD-WvkgrRIIo","outputId":"c5528b11-e551-47be-e2b2-812769daaa0e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** \n\n* The authorized Flag also doesn't give a suspectble change in the IQR range between authorized and un_authorized transactions.\n\n* Even for the un_authorized transactions card users have same IQR. Because of the many transactions by an user, these un_authorized doesn't have much effect.\n\n* But this categorical features also should be included using response coding (to represent the categorical data, probability of the data point belonging to a particular class given a category).","metadata":{"id":"7mjgtD-wRIIo"}},{"cell_type":"markdown","source":"**Analysis of installments feature :**","metadata":{"id":"1H385Y9aRIIr"}},{"cell_type":"code","source":"print('Quantile values for installments in Historical Transaction :')\nprint('25th Percentile :',historical_data['installments'].quantile(0.25))\nprint('50th Percentile :',historical_data['installments'].quantile(0.50))\nprint('75th Percentile :',historical_data['installments'].quantile(0.75))\nprint('100th Percentile :',historical_data['installments'].quantile(1))\nprint('\\n******************************************************************\\n')\nprint('Quantile values for installments in New Merchant Transaction :')\nprint('25th Percentile :',newmerchant_data['installments'].quantile(0.25))\nprint('50th Percentile :',newmerchant_data['installments'].quantile(0.50))\nprint('75th Percentile :',newmerchant_data['installments'].quantile(0.75))\nprint('100th Percentile :',newmerchant_data['installments'].quantile(1))","metadata":{"id":"bIpfuruFRIIr","outputId":"76f2b42d-e121-49af-cd5d-3c99e11465db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of target over installment feature :","metadata":{"id":"Ndqj-yL9RIIr"}},{"cell_type":"code","source":"plt.figure(figsize = (14,5))\nplt.subplot(121)\nsns.boxplot(x=historical_data[\"installments\"])\nplt.title(\"Distributions of installments (historical_transactions)\")\nplt.subplot(122)\nsns.boxplot(x=newmerchant_data[\"installments\"])\nplt.title(\"Distributions of installments (new_merchant_transactions)\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data[\"installments\"].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\nplt.subplot(121)\nsns.boxplot(y='target',x= 'installments', data = historical_data)\nplt.title(\"Distrbutions of target over installments(historical_transactions)\")\nplt.subplot(122)\nsns.boxplot(y='target',x = 'installments', data = newmerchant_data)\nplt.title(\"Distrbutions of target over installments(new_merchant_transactions)\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"cT251YafRIIs","outputId":"d768180c-2d08-4f28-b68d-8b27ff68f15b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** The installments also have outliers, these outliers should be taken care in data preprocessing. In historical_transactions and new_merchants_transactions the 75% of installments are below 1. So, most of the payments through the cards are instant payments or short term installments.","metadata":{"id":"FCoD_NRSRIIs"}},{"cell_type":"markdown","source":"**Analysis of purchase_amount feature :**","metadata":{"id":"Y0ykAqgHRIIp"}},{"cell_type":"code","source":"print('Quantile values for purchase amount in Historical Transaction :')\nprint('25th Percentile :',historical_data['purchase_amount'].quantile(0.25))\nprint('50th Percentile :',historical_data['purchase_amount'].quantile(0.50))\nprint('75th Percentile :',historical_data['purchase_amount'].quantile(0.75))\nprint('100th Percentile :',historical_data['purchase_amount'].quantile(1))\nprint('\\n******************************************************************\\n')\nprint('Quantile values for purchase amount in New Merchant Transaction :')\nprint('25th Percentile :',newmerchant_data['purchase_amount'].quantile(0.25))\nprint('50th Percentile :',newmerchant_data['purchase_amount'].quantile(0.50))\nprint('75th Percentile :',newmerchant_data['purchase_amount'].quantile(0.75))\nprint('100th Percentile :',newmerchant_data['purchase_amount'].quantile(1))","metadata":{"id":"twnJ6GOxRIIp","outputId":"a8c9c681-261f-48b0-990a-6e7accd97cf7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** The IQR range value is very small. And there is one outlier which have 6010603.9717525. These outlier can skew the final model performance. purchase_amount is normalized. Let's have a look at it nevertheless.","metadata":{"id":"M4MCmPt7RIIp"}},{"cell_type":"code","source":"plt.figure(figsize = (13,5))\nplt.subplot(121)\nplt.title('Purchase amount (Historical Transaction)');\nhistorical_data['purchase_amount'].plot(kind='hist');\nplt.subplot(122)\nplt.title('Purchase amount (NewMerchant Transaction)');\nnewmerchant_data['purchase_amount'].plot(kind='hist');","metadata":{"id":"SRWhc42FRIIp","outputId":"ac64203d-7e2f-4c07-d192-69e955f24d58","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data['purchase_amount'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('For purchase_amount in Historical transactions :')\nfor i in [-1, 0]:\n    n = historical_data.loc[historical_data['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = historical_data.loc[historical_data['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")\n    \nprint(70 * '-')\n\nprint('For purchase_amount in New Merchant transactions :')\nfor i in [-1, 0]:\n    n = newmerchant_data.loc[newmerchant_data['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = newmerchant_data.loc[newmerchant_data['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")","metadata":{"id":"ju-XeAl0RIIq","outputId":"b379e744-1972-4e43-c9ee-516dd0b0b2e2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** As we can see the major chunk of transactions has purchase_amount less than 0. let us see Purchase amount distribution for negative values.","metadata":{"id":"Vg2-P0jYRIIq"}},{"cell_type":"code","source":"plt.figure(figsize = (14,5))\nplt.subplot(121)\nplt.title(' Negative purchase_amount distribution (Historical)');\nhistorical_data.loc[historical_data['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');\nplt.subplot(122)\nplt.title('Negative purchase_amount distribution (New Merchant)');\nnewmerchant_data.loc[newmerchant_data['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');","metadata":{"id":"kQJKv9CXRIIq","outputId":"0fa917e0-0d54-4978-8bbc-4d9562214277","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** It seems that almost all transactions have purchase amount in range (-1, 0). Quite a strong normalization and high outliers, which will need to be processed.","metadata":{"id":"aouPEB3-RIIr"}},{"cell_type":"markdown","source":"Now, let's see purchase_amount feature over target variable :\n\n","metadata":{"id":"88c_gZSfXm7h"}},{"cell_type":"code","source":"# Before removing detected outlier in 'historical_data'\nhistorical_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#There is one outlier which have value 6010603.9717525. We will remove it for further EDA.\nhistorical_data = historical_data[historical_data['purchase_amount']  != 6010603.9717525]","metadata":{"id":"l1FVmi9xXrUh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After removal from 'historical_data'\nhistorical_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.scatterplot(data=historical_data, x=\"purchase_amount\", y=\"target\")\nplt.title(\"purchase_amount (historical_transaction) over target\")\nplt.subplot(122)\nsns.scatterplot(data=newmerchant_data, x=\"purchase_amount\", y=\"target\")\nplt.title(\"purchase_amount (newmerchant_transaction) over target\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"wErUAjH5XxuN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* One key observation here is, Most of the outliers in target having value around -30 are having very less purchase amount.\n* With the increase in purchase amount customer become more loyal, as target score increases.???  Target loyalty score decrease with more purchase amount???","metadata":{"id":"VMyHBcP2Xmv5"}},{"cell_type":"markdown","source":"**Analysis of feature Month_lag :**","metadata":{"id":"70w1LSM7RIIt"}},{"cell_type":"code","source":"plt.figure(figsize = (13,5))\nplt.subplot(121)\nplt.title('Month lag (Historical Transaction)');\nhistorical_data['month_lag'].plot(kind='hist');\nplt.subplot(122)\nplt.title('Month lag (NewMerchant Transaction)');\nnewmerchant_data['month_lag'].plot(kind='hist');","metadata":{"id":"vDcpjdq-RIIt","outputId":"75c5de66-9024-4181-eee2-bc16eab41354","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of target over month_lag feature :","metadata":{"id":"tr5WwjsZAHh7"}},{"cell_type":"code","source":"plt.figure(figsize = (14,5))\nplt.subplot(121)\nsns.boxplot(y= 'target',x= 'month_lag', data = historical_data)\nplt.title(\"Distrbutions of target over month_lag (historical_transactions)\")\n\nplt.subplot(122)\nsns.boxplot(y= 'target',x= 'month_lag', data = newmerchant_data)\nplt.title(\"Distrbutions of target over month_lag (historical_transactions)\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"Awt3xvqtRIIv","outputId":"3d21f0c4-a75b-4253-aeff-168a6db4a612","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** \n\n* The Month_lag gives important info to predict the loyalty score. For a Purchase in installments, how many months the card lags from the actual end date of installment is the month_lag feature.\n\n* The historical_transactions have month_lags from 0 to 13. which means the cards with transactions in histortical_transactions data have lag of installments from 0 to 13. But, the new_merchant_transactions have month_lag 1 and 2 only.\n\n* This again proves the difference in the transactions type between the historical and new merchants.","metadata":{"id":"1w1-CzINRIIv"}},{"cell_type":"code","source":"newmerchant_data['month_lag'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis of feature 'purchase_date' :**","metadata":{"id":"jlNI3u1cRIIv"}},{"cell_type":"markdown","source":"At first, we convert purchase_date to datetime format :","metadata":{"id":"wGn_wPcVRIIw"}},{"cell_type":"code","source":"historical_data['purchase_date'] = pd.to_datetime(historical_data['purchase_date'],\n                                                  format='%Y-%m-%d %H:%M:%S')\nnewmerchant_data['purchase_date'] = pd.to_datetime(newmerchant_data['purchase_date'],\n                                                   format='%Y-%m-%d %H:%M:%S')","metadata":{"id":"iyL-jlJvRIIw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of transactions vs Year :","metadata":{"id":"OgSbEjLARsv2"}},{"cell_type":"code","source":"#barplot for the Number of transactions vs Year\nfig, ax = plt.subplots(1, 2, figsize = (14, 5));\nhistorical_data['purchase_date'].dt.year.value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal',\n                                                                          title='Transactions Vs Year (histortical_transactions)', rot=0)\nnewmerchant_data['purchase_date'].dt.year.value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown',\n                                                                           title='Transactions Vs Year (new_merchant transactions)', rot=0)\n\n\nprint('Year-Wise Percentage distribution of purchase_date (Historical-Transaction) :')\nprint(historical_data['purchase_date'].dt.year.value_counts(normalize = True)*100)\nprint('\\nYear-Wise Percentage distribution of purchase_date (NewMerchant-Transaction) :')\nprint(newmerchant_data['purchase_date'].dt.year.value_counts(normalize = True)*100)","metadata":{"id":"FO7U1sE-CB2n","outputId":"6cdbbe3b-d2df-4197-a89c-2e6cd215e013","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* In historical_transactions, The transactions with respect to year 2017 is way more (~82%) than transactions in 2018 (18%). \n\n* But, In new_merchant_transactions, transactions with respect to 2018 is way more (~85%) than transactions in 2018 (15%).\n\n* Then we can say, new_merchant_transactions are the recent year transactions. This is the reason for the disparity in the purchase amount and installment features.","metadata":{"id":"oWVa5g6SRIIw"}},{"cell_type":"markdown","source":"Number of transactions vs Week","metadata":{"id":"zfbYF9ECRm3I"}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (15, 5));\nhistorical_data['purchase_date'].dt.dayofweek.value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal',\n                                                                               title='Transactions Vs dayofweek (histortical_transactions)', rot=0);\nnewmerchant_data['purchase_date'].dt.dayofweek.value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown',\n                                                                                title='Transactions Vs dayofweek (new_merchant_transactions)', rot=0);","metadata":{"id":"ELAn8mf8SQtp","outputId":"ca7ce5ea-54d9-42fc-d127-b03e22fc9e8a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of target over dayofweek :","metadata":{"id":"EDWm6l71HVq7"}},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\nplt.subplot(121)\nsns.boxplot(y = historical_data['target'], x = historical_data['purchase_date'].dt.dayofweek)\nplt.xticks(range(0,7),labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\nplt.xlabel('Days of week')\nplt.title(\"Distribution of target over dayofweek (histortical_transactions)\")\n\nplt.subplot(122)\nsns.boxplot(y = historical_data['target'], x = newmerchant_data['purchase_date'].dt.dayofweek)\nplt.xticks(range(0,7),labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\nplt.xlabel('Days of week')\nplt.title(\"Distribution of target over dayofweek (new_merchant_transactions)\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"4Z5MMCleRIIx","outputId":"b4da1c26-91d3-4ef3-f720-a5d9c1d2cb26","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of transactions vs hour","metadata":{"id":"B5Xy6s_aRbwS"}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (15, 5));\nhistorical_data['purchase_date'].dt.hour.value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal',\n                                                                          title='Transactions Vs hour (histortical_transactions)', rot=0);\nnewmerchant_data['purchase_date'].dt.hour.value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown',\n                                                                           title='Transactions Vs hour (new_merchant_transactions)', rot=0);","metadata":{"id":"KRYzxGR0PSd1","outputId":"74db0d81-3d40-4f00-e6b2-33e379177485","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of target over hour :","metadata":{"id":"EqR_NBK-HFcL"}},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.boxplot(y = historical_data['target'], x = historical_data['purchase_date'].dt.hour)\nplt.xlabel('Hour')\nplt.xticks(range(0,24))\nplt.title(\"Distribution of target over hour (histortical_transactions)\")\n\nplt.subplot(122)\nsns.boxplot(y = historical_data['target'], x = newmerchant_data['purchase_date'].dt.hour)\nplt.xlabel('Hour')\nplt.xticks(range(0,24))\nplt.title(\"Distribution of target over hour (new_merchant_transactions)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"Up1a0KSYRIIx","outputId":"87359542-48f2-4ab2-e1e3-30b0fb3bc4ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* From the distribution of both weekly and hourly transactions count, these transactions have not much difference in their distributions.\n\n* Since, the data given in the problem is a generated data and not a real time data. The distribution of the transactions over the purchase date is similar.\n\n* But, the type of transactions differs from historical and new_merchants in terms of purchase_amount, month_lag and installments.\n\n* By checking the number of merchants are in both historical and new_merchants transactions, we can get exclusive informations of the merchants.","metadata":{"id":"vhGXPDHgRIIy"}},{"cell_type":"markdown","source":"**Let's create a feature called Number of transactions for each card_id and see - How it impacts target variable ?**\n\nNumber of Transactions feature is not explicitly given in any of the file but we can derive it with some hacks :","metadata":{"id":"D6pIeWt_pHol"}},{"cell_type":"code","source":"# For historical transactions\ng = historical_data[['card_id']].groupby('card_id')\ndf_transaction_counts = g.size().reset_index(name='num_transactions')\nhistorical_data = pd.merge(historical_data ,df_transaction_counts, on=\"card_id\",how='left')\nhistorical_data.head()","metadata":{"id":"uB1hMVRA323N","outputId":"e2f630c6-0b24-4b1a-8363-ba935a259001","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"historical_data['num_transactions'].describe()","metadata":{"id":"RszZR_rANxCX","outputId":"fbdf69a7-2292-433c-9702-ba54a061bd89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For New Merchant transaction\ng = newmerchant_data[['card_id']].groupby('card_id')\ndf_transaction_counts = g.size().reset_index(name='num_transactions')\nnewmerchant_data = pd.merge(newmerchant_data ,df_transaction_counts, on=\"card_id\",how='left')\nnewmerchant_data.head()","metadata":{"id":"3LPGsfTzpGgW","outputId":"e0bc5835-b6e9-4497-8646-860c91166c6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newmerchant_data['num_transactions'].describe()","metadata":{"id":"M2hDPkdCO16w","outputId":"e2af3b24-3599-4cf1-dbeb-0f209423041a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.scatterplot(data=historical_data, x=\"num_transactions\", y=\"target\")\nplt.title(\"Number of transactions (historical_transaction) VS target\")\nplt.subplot(122)\nsns.scatterplot(data=newmerchant_data, x=\"num_transactions\", y=\"target\")\nplt.title(\"Number of Transactions (newmerchant_transaction) VS target\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"tZhj5upNfdFC","outputId":"c65e099c-714f-41d8-8173-5c7cc1c622a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* One key observation here is, Most of the outliers in target having value around -30 are having very less no of transactions.\n* With increase in no of transactions customer become more loyal, as target score increases???","metadata":{"id":"eMyJyjdt95Wu"}},{"cell_type":"markdown","source":"**Correlation between variables : Variance Inflation Factor**","metadata":{"id":"oKPHANHCbhE_"}},{"cell_type":"code","source":"selected_columns = ['category_2','month_lag','purchase_amount','state_id','subsector_id', 'installments']\ndata_frame = newmerchant_data[selected_columns]\n\ndata_frame = data_frame.dropna()\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(data_frame.iloc[:,:].values, i) for i in range(data_frame.shape[1])]\nvif[\"features\"] = data_frame.columns\nvif","metadata":{"id":"N-qbcMjBJ_Ss","outputId":"64d5f9d6-d8ec-4d0e-90af-bf01be83cb9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All values are under 10, let's add some more features and again we'll calculate the VIF :\n\n\n","metadata":{"id":"Bi_vtrTYLDUq"}},{"cell_type":"code","source":"Dict = {'A':1,'B':2,'C':3}\nDict1 = {'Y':1,'N':0}\n\nselected_columns = ['authorized_flag','category_3','category_2','month_lag','purchase_amount','state_id','subsector_id', 'installments']\ndata_frame = newmerchant_data[selected_columns]\ndata_frame['category_3'] = data_frame['category_3'].map(Dict)\ndata_frame['authorized_flag'] = data_frame['authorized_flag'].map(Dict1)\n\ndata_frame = data_frame.dropna()\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(data_frame.iloc[:,:].values, i) for i in range(data_frame.shape[1])]\nvif[\"features\"] = data_frame.columns\nvif","metadata":{"id":"XJYpygregZfI","outputId":"918bf4b4-b787-4bd5-8884-0fec136ce3d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* The value for the authorized flag is somewhat higher, it is around 32 which indicates possible correlation. So this variable needs further investigation.\n\n* Other than the authorized flag the remaining variables doesn't look correlated. They are well under 2.","metadata":{"id":"TTXyFIzFRIIz"}},{"cell_type":"markdown","source":"<h2>Exploring the Merchant Data :</h2>\n\n","metadata":{"id":"jPWXcMhZRIIz"}},{"cell_type":"code","source":"# Read excel-formatted data dictionary file with pandas\n#data_dictionary = pd.read_excel('../input/elo-merchant-category-recommendation/Data_Dictionary.xlsx', sheet_name='merchant')\n#data_dictionary","metadata":{"id":"DvNMlL7wRIIz","outputId":"3aea4bee-0c82-411b-8e27-2f51ee32b213","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data.head()","metadata":{"id":"rfKLFZC4RIIz","outputId":"05795d23-420a-4102-f957-3c9d5332a56a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data.info()","metadata":{"id":"QvQcFgBARII0","outputId":"a6d08418-0e90-4fbf-99ec-c569c463821e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchants_data.isna().any()","metadata":{"id":"1xIhqTqdRII0","outputId":"bcaf6ad7-bb67-4649-9b1a-335db63f80e3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** Merchant data has missing values in columns : avg_sales_lag3, avg_sales_lag6 and avg_sales_lag12 ","metadata":{"id":"2UlevBw9RII0"}},{"cell_type":"markdown","source":"**Analysis of Numerical features : numerical_1 and numerical_2**","metadata":{"id":"zqvHbkdiRII1"}},{"cell_type":"code","source":"print('Quantile values for numeric_1 in Transaction data:')\nprint('25th Percentile :',merchants_data['numerical_1'].quantile(0.25))\nprint('50th Percentile :',merchants_data['numerical_1'].quantile(0.50))\nprint('75th Percentile :',merchants_data['numerical_1'].quantile(0.75))\nprint('100th Percentile :',merchants_data['numerical_1'].quantile(1))\nprint('\\n******************************************************************\\n')\nprint('Quantile values for numeric_2 in Transaction data:')\nprint('25th Percentile :',merchants_data['numerical_2'].quantile(0.25))\nprint('50th Percentile :',merchants_data['numerical_2'].quantile(0.50))\nprint('75th Percentile :',merchants_data['numerical_2'].quantile(0.75))\nprint('100th Percentile :',merchants_data['numerical_2'].quantile(1))","metadata":{"id":"PjYxBupKRII2","outputId":"f8a93c22-7393-4a6b-89e2-fbe3fb35714f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** I think the Distribution of numerical_1 and numerical_2 featurs are almost identical, because three quantiles have identical values.","metadata":{"id":"fBfsB-f6RII2"}},{"cell_type":"code","source":"plt.figure(figsize=(12,5) )\nplt.subplot(121)\nsns.kdeplot(np.log10(merchants_data['numerical_1']),shade=True)\nplt.title(\"PDF of numerical_1 in LogScale\")\nplt.xlabel('log (numerical_1)')\nplt.subplot(122)\nsns.kdeplot(np.log10(merchants_data['numerical_2']),shade=True)\nplt.title(\"PDF of numerical_2 in LogScale\")\nplt.xlabel('log (numerical_2)')\nplt.tight_layout()\nplt.show()","metadata":{"id":"hqc_xaDQrAFa","outputId":"e76ced5a-4d44-4975-f64c-667da0b1dc61","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** After plotting PDF, it is very clear that both the features have same distribution, may be they are duplicates of each other.\n\n**Note :** The values for numeric_1 and numeric_2 are mostly -ve and very near to zero. So, I preferred LogScale for analysis.","metadata":{"id":"ke9LcwUA4IQj"}},{"cell_type":"markdown","source":"**Analysis of the three anonymized category features : category_1,category_2 and category_4**","metadata":{"id":"dkLyz7J6RII0"}},{"cell_type":"code","source":"print('Value counts for category features of Merchants data :\\n')\nprint(merchants_data['category_1'].value_counts())\nprint('******************************')\nprint(merchants_data['category_2'].value_counts())\nprint('******************************')\nprint(merchants_data['category_4'].value_counts())\n\nfig, ax = plt.subplots(1, 3, figsize = (15, 5));\nmerchants_data['category_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='category_1', rot=0);\nmerchants_data['category_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='category_2', rot=0);\nmerchants_data['category_4'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='category_3', rot=0);\nplt.suptitle('Counts for category features of Merchants_data');","metadata":{"id":"Hfr9itTaRII1","outputId":"2d4f2d24-3867-41fc-cf2f-0c3838b0f059","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** These are anonymous categories, which can represent some properties of the merchants, which is still unclear after merging with the transactions data it can reveal more info.","metadata":{"id":"kU3lcmi8RII1"}},{"cell_type":"markdown","source":"**Analysis of 'feature most_recent_sales_range' and 'most_recent_purchases_range' :**","metadata":{"id":"K5n1fqHGRII6"}},{"cell_type":"code","source":"print(merchants_data['most_recent_sales_range'].value_counts())\nprint('*******************************************')\nprint(merchants_data['most_recent_purchases_range'].value_counts())\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 5));\nmerchants_data['most_recent_sales_range'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal',\n                                                                           title='most_recent_sales_range', rot=0);\nmerchants_data['most_recent_purchases_range'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown',\n                                                                               title='most_recent_purchases_range', rot=0);","metadata":{"id":"qsPEX0Y1RII6","outputId":"4e07c57d-ef5f-4b1d-ba60-6138f225ff0a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :**\n\n* Both the features have very similar distributions.\n\n* The sales range in last active month is a categorical feature with \"A\",\"B\",\"C\",\"D\",\"E\". after observing the trend from graph we can say Range of revenue (monetary units) is in order E > D > C > B > A.\n\n* The Bar Plot shows there are many merchants with revenue range of \"E\" than other ranges.\n\n* And also, Bar Plot shows there are many merchants with purchase quantity range of \"E\" than other ranges.\n\n* The sales range and purchase range can be used in aggregated to know the card_id's most visited merchants in the final features for training.","metadata":{"id":"EgW0PfpfRII7"}},{"cell_type":"markdown","source":"**Analysis of Sales Average features :\n'avg_sales_lag3', 'avg_purchases_lag6', 'avg_sales_lag12', 'avg_purchases_lag3', 'avg_purchases_lag6', 'avg_purchases_lag12'**\n","metadata":{"id":"hL3BmsZiRII2"}},{"cell_type":"code","source":"print('Quantile values for avg_sales_lag3 in Transaction data:')\nprint('25th Percentile :',merchants_data['avg_sales_lag3'].quantile(0.25))\nprint('50th Percentile :',merchants_data['avg_sales_lag3'].quantile(0.50))\nprint('75th Percentile :',merchants_data['avg_sales_lag3'].quantile(0.75))\nprint('100th Percentile :',merchants_data['avg_sales_lag3'].quantile(1))\nprint('\\n******************************************************************\\n')\nprint('Quantile values for avg_sales_lag6 in Transaction data:')\nprint('25th Percentile :',merchants_data['avg_sales_lag6'].quantile(0.25))\nprint('50th Percentile :',merchants_data['avg_sales_lag6'].quantile(0.50))\nprint('75th Percentile :',merchants_data['avg_sales_lag6'].quantile(0.75))\nprint('100th Percentile :',merchants_data['avg_sales_lag6'].quantile(1))\nprint('Quantile values for numeric_1 in Transaction data6:')\nprint('\\n******************************************************************\\n')\nprint('Quantile values for avg_sales_lag12 in Transaction data:')\nprint('25th Percentile :',merchants_data['avg_sales_lag12'].quantile(0.25))\nprint('50th Percentile :',merchants_data['avg_sales_lag12'].quantile(0.50))\nprint('75th Percentile :',merchants_data['avg_sales_lag12'].quantile(0.75))\nprint('100th Percentile :',merchants_data['avg_sales_lag12'].quantile(1))","metadata":{"id":"QOoiTXX3RII3","outputId":"ea6dfab0-6990-449c-f601-01092cd7a487","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Statistical insights for avg_purchases_lag3 in Transaction data:')\nprint(merchants_data['avg_purchases_lag3'].describe())\nprint('\\n******************************************************************\\n')\nprint('Statistical insights for avg_purchases_lag6 in Transaction data:')\nprint(merchants_data['avg_purchases_lag6'].describe())\nprint('\\n******************************************************************\\n')\nprint('Statistical insights for avg_purchases_lag12 in Transaction data:')\nprint(merchants_data['avg_purchases_lag12'].describe())","metadata":{"id":"BZuwFwcVRII3","outputId":"63a806ba-aed8-4503-a99e-430c02bab377","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** There are outliers with the value inf in each of these columns, we have to deal with it. For EDA part, I am removing the corresponding rows with the inf values in the columns avg_purchases_lag3, avg_purchases_lag6, avg_purchases_lag12. We will see what else we can do with these outliers in preprocessing part.","metadata":{"id":"MW9MVL2GRII4"}},{"cell_type":"code","source":"merchants_data = merchants_data[merchants_data['avg_purchases_lag3']  != np.inf]\nmerchants_data = merchants_data[merchants_data['avg_purchases_lag6']  != np.inf]\nmerchants_data = merchants_data[merchants_data['avg_purchases_lag12']  != np.inf]","metadata":{"id":"3ODZn-fcRII4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplot(231)\nsns.kdeplot(np.log10(merchants_data['avg_sales_lag3']),shade=True)\nplt.title(\"PDF of avg_sales_lag3 in LogScale\")\nplt.xlabel('log(avg_sales_lag3)')\nplt.subplot(232)\nsns.kdeplot(np.log10(merchants_data['avg_sales_lag6']),shade=True)\nplt.title(\"PDF of avg_sales_lag6 in LogScale\")\nplt.xlabel('log(avg_sales_lag6)')\nplt.subplot(233)\nsns.kdeplot(np.log10(merchants_data['avg_sales_lag12']),shade=True)\nplt.title(\"PDF of avg_sales_lag12 in LogScale\")\nplt.xlabel('log(avg_sales_lag12)')\nplt.subplot(234)\nsns.kdeplot(np.log10(merchants_data['avg_purchases_lag3']),shade=True)\nplt.title(\"PDF of avg_purchases_lag3 in LogScale\")\nplt.xlabel('log(avg_purchases_lag3)')\nplt.subplot(235)\nsns.kdeplot(np.log10(merchants_data['avg_purchases_lag6']),shade=True)\nplt.title(\"PDF of avg_purchases_lag6 in LogScale\")\nplt.xlabel('log(avg_purchases_lag6)')\nplt.subplot(236)\nsns.kdeplot(np.log10(merchants_data['avg_purchases_lag12']),shade=True)\nplt.title(\"PDF of avg_purchases_lag12 in LogScale\")\nplt.xlabel('log(avg_purchases_lag12)')\nplt.tight_layout()\nplt.show()","metadata":{"id":"DoPeRxH06FSD","outputId":"be4933a3-c905-48c0-a4ea-aae11063970d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** \n\n* The average purchases and sales across 3, 6 and 12 months are distributed near 1.\n\n* And, there are outliers in all the average sales and purchases. These features gives info about the merchants but not about the card_id's. The information about the merchants have to cumulated for each card_id's.\n\n**Note :** The values for All the sales features listed above are mostly surrounded very near to 1. So, I preferred LogScale for analysis.","metadata":{"id":"OPAlZ7ysRII5"}},{"cell_type":"markdown","source":"**Quantity of active months : Analysis of features ('active_months_lag3', 'active_months_lag6' and 'active_months_lag12') :**","metadata":{"id":"e7qFvYzxRII5"}},{"cell_type":"code","source":"print(merchants_data['active_months_lag3'].value_counts())\nprint('**********************************')\nprint(merchants_data['active_months_lag6'].value_counts())\nprint('**********************************')\nprint(merchants_data['active_months_lag12'].value_counts())\n\nfig, ax = plt.subplots(1, 3, figsize = (15, 5));\nmerchants_data['active_months_lag3'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal',\n                                                                      title='active_months_lag3', rot=0);\nmerchants_data['active_months_lag6'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown',\n                                                                      title='active_months_lag6', rot=0);\nmerchants_data['active_months_lag12'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold',\n                                                                       title='active_months_lag12', rot=0);\nplt.suptitle('Counts of Active month lags');","metadata":{"id":"FBhvua_ARII6","outputId":"543df8dc-8e5f-458d-b9c0-b86e82beec53","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** The active months features are greatly skewed and doesn't provide any vital information about the cards.","metadata":{"id":"aTt19IOURII6"}},{"cell_type":"markdown","source":"**Correlation between variables : Variance Inflation Factor**","metadata":{"id":"BBGbGPBsRII7"}},{"cell_type":"code","source":"selected_columns = ['numerical_1', 'numerical_2','category_2','avg_sales_lag3','avg_sales_lag6','avg_sales_lag12','avg_purchases_lag3','avg_purchases_lag6','avg_purchases_lag12','active_months_lag3','active_months_lag6','active_months_lag12']\ndata_frame = merchants_data[selected_columns]\ndata_frame = data_frame.dropna()\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(data_frame.iloc[:,:].values, i) for i in range(data_frame.shape[1])]\nvif[\"features\"] = data_frame.columns\nvif","metadata":{"id":"k1pl4mVuHXp4","outputId":"10ffd0a4-b0f5-4f84-b145-b33be61667a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation :** Looks like there are variables which are heavily correlated like 'active_months_lag6', 'avg_purchase_lag6' and 'avg_sales_lag_6' and 'avg_purchase_lag12' and as we seen before the 'numerical_1' and 'numerical_2' have similar values and distributions and they are correlated.\n\n","metadata":{}},{"cell_type":"markdown","source":"**TOTAL OBSERVATIONS :**\n\n1) Target variable i.e. Loyalty scores are real-numbers, It directly gives us the intuition that we have to go for a supervised machine learning regression model to solve this problem.\n\n2) The data files are train, test, new_merchant, merchant and historical transactions. but datasets are largely anonymized, and the meaning of the features are not elaborated.\n\n3) The dimensionality of train and test data is very less. That clearly shows that the information provided is not sufficient for training. As only three features have been given in the train file which seems to be not sufficient to make good predictions. More features must be added to this with the help of domain knowledge and the business problem given.\n\n4) Distribution of both the train and test are almost identical. So there is no time based splitting in the make over of the data. And, it assures for prediction of the test data.\n\n5) The target variable is normally distributed but, there are outliers which seems to be accumulated around -30.\n\n6) Data is not complete as nan values are present in the merchants, historical and new merchants transactions, so these missing values must be imputed for better predciton.\n\n7) One-hot encoding/response coding of categorical features should be done for better prediction. The categorical features present across dataset are large in number than numerical features. \n\n8) Merchants data have high number of correlated features in it as compared to other data files. This is suggested by the calcuation of the VIF Scores \n\n9) The time features can reveal the inherent property of the transactions and the transactions are time dependent, the engineered features from the features like purchase_date will be useful in prediction.\n\n10) In the historical transactions data there is this feature called 'authorized_flag' count which indicates whether the transaction is authorized or not. There is very less number of transactions which is not authorized. Considering this flag features as a separater in the feature engineering can results can give better prediction.\n\nAt the End of the Exploration of the transactions, merchants and train data, the given features of transactions are not big factor for the calculation of the target Score.\n\nThere exist an aggregrated or engineered feature or features which can be helpful in predicting the target Score.\n\nWith the different feature engineering techniques and market research techniques we have to produce the new features which may or may not be very useful in the prediction model.\n\nBy implementing the major feature engineering ideas we have to produce features and build model upon it.","metadata":{}}]}